# Comparing `tmp/codeflare_torchx-0.5.0.dev5-py3-none-any.whl.zip` & `tmp/codeflare_torchx-0.6.0.dev0-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,115 +1,119 @@
-Zip file size: 225809 bytes, number of entries: 113
+Zip file size: 240917 bytes, number of entries: 117
 -rw-rw-r--  2.0 unx      340 b- defN 23-Mar-31 21:33 torchx/__init__.py
 -rw-rw-r--  2.0 unx      993 b- defN 23-Mar-31 21:33 torchx/notebook.py
--rw-rw-r--  2.0 unx      935 b- defN 23-Apr-03 18:34 torchx/version.py
+-rw-rw-r--  2.0 unx      936 b- defN 23-Apr-10 16:08 torchx/version.py
 -rw-rw-r--  2.0 unx      231 b- defN 23-Mar-31 21:33 torchx/apps/__init__.py
 -rw-rw-r--  2.0 unx      208 b- defN 23-Mar-31 21:33 torchx/apps/serve/__init__.py
 -rw-rw-r--  2.0 unx     4371 b- defN 23-Mar-31 21:33 torchx/apps/serve/serve.py
 -rw-rw-r--  2.0 unx      208 b- defN 23-Mar-31 21:33 torchx/apps/utils/__init__.py
 -rw-rw-r--  2.0 unx     1412 b- defN 23-Mar-31 21:33 torchx/apps/utils/booth_main.py
 -rw-rw-r--  2.0 unx     1823 b- defN 23-Mar-31 21:33 torchx/apps/utils/copy_main.py
 -rw-rw-r--  2.0 unx     3437 b- defN 23-Mar-31 21:33 torchx/apps/utils/process_monitor.py
 -rw-rw-r--  2.0 unx    10336 b- defN 23-Mar-31 21:33 torchx/cli/__init__.py
--rw-rw-r--  2.0 unx     2947 b- defN 23-Mar-31 21:33 torchx/cli/argparse_util.py
+-rw-rw-r--  2.0 unx     2836 b- defN 23-Apr-10 16:06 torchx/cli/argparse_util.py
 -rw-rw-r--  2.0 unx      775 b- defN 23-Mar-31 21:33 torchx/cli/cmd_base.py
 -rw-rw-r--  2.0 unx      820 b- defN 23-Mar-31 21:33 torchx/cli/cmd_cancel.py
--rw-rw-r--  2.0 unx     1708 b- defN 23-Mar-31 21:33 torchx/cli/cmd_configure.py
+-rw-rw-r--  2.0 unx     1707 b- defN 23-Apr-10 16:06 torchx/cli/cmd_configure.py
 -rw-rw-r--  2.0 unx     1268 b- defN 23-Mar-31 21:33 torchx/cli/cmd_describe.py
 -rw-rw-r--  2.0 unx     1414 b- defN 23-Mar-31 21:33 torchx/cli/cmd_list.py
 -rw-rw-r--  2.0 unx     6003 b- defN 23-Mar-31 21:33 torchx/cli/cmd_log.py
--rw-rw-r--  2.0 unx    10507 b- defN 23-Mar-31 21:33 torchx/cli/cmd_run.py
+-rw-rw-r--  2.0 unx    10417 b- defN 23-Apr-10 16:06 torchx/cli/cmd_run.py
 -rw-rw-r--  2.0 unx     1287 b- defN 23-Mar-31 21:33 torchx/cli/cmd_runopts.py
--rw-rw-r--  2.0 unx     1827 b- defN 23-Mar-31 21:33 torchx/cli/cmd_status.py
--rw-rw-r--  2.0 unx     6183 b- defN 23-Mar-31 21:33 torchx/cli/cmd_tracker.py
+-rw-rw-r--  2.0 unx     1821 b- defN 23-Apr-10 16:06 torchx/cli/cmd_status.py
+-rw-rw-r--  2.0 unx     5203 b- defN 23-Apr-10 16:06 torchx/cli/cmd_tracker.py
 -rw-rw-r--  2.0 unx      553 b- defN 23-Mar-31 21:33 torchx/cli/colors.py
--rw-rw-r--  2.0 unx     3442 b- defN 23-Mar-31 21:33 torchx/cli/main.py
+-rw-rw-r--  2.0 unx     3469 b- defN 23-Apr-10 16:06 torchx/cli/main.py
 -rw-rw-r--  2.0 unx    12106 b- defN 23-Mar-31 21:33 torchx/components/__init__.py
--rw-rw-r--  2.0 unx     3995 b- defN 23-Mar-31 21:33 torchx/components/component_test_base.py
--rw-rw-r--  2.0 unx     9521 b- defN 23-Mar-31 21:33 torchx/components/dist.py
+-rw-rw-r--  2.0 unx     4135 b- defN 23-Apr-10 16:06 torchx/components/component_test_base.py
+-rw-rw-r--  2.0 unx    14284 b- defN 23-Apr-10 16:06 torchx/components/dist.py
 -rw-rw-r--  2.0 unx      697 b- defN 23-Mar-31 21:33 torchx/components/interpret.py
 -rw-rw-r--  2.0 unx     2814 b- defN 23-Mar-31 21:33 torchx/components/metrics.py
 -rw-rw-r--  2.0 unx     2141 b- defN 23-Mar-31 21:33 torchx/components/serve.py
+-rw-rw-r--  2.0 unx     9542 b- defN 23-Apr-10 16:06 torchx/components/structured_arg.py
 -rw-rw-r--  2.0 unx     1259 b- defN 23-Mar-31 21:33 torchx/components/train.py
 -rw-rw-r--  2.0 unx     9025 b- defN 23-Mar-31 21:33 torchx/components/utils.py
 -rw-rw-r--  2.0 unx      208 b- defN 23-Mar-31 21:33 torchx/components/integration_tests/__init__.py
--rw-rw-r--  2.0 unx     3925 b- defN 23-Mar-31 21:33 torchx/components/integration_tests/component_provider.py
+-rw-rw-r--  2.0 unx     3980 b- defN 23-Apr-10 16:06 torchx/components/integration_tests/component_provider.py
 -rw-rw-r--  2.0 unx     5150 b- defN 23-Mar-31 21:33 torchx/components/integration_tests/integ_tests.py
+-rw-rw-r--  2.0 unx     8786 b- defN 23-Apr-10 16:06 torchx/distributed/__init__.py
 -rw-rw-r--  2.0 unx        0 b- defN 23-Mar-31 21:33 torchx/examples/__init__.py
 -rw-rw-r--  2.0 unx        0 b- defN 23-Mar-31 21:33 torchx/examples/apps/__init__.py
 -rw-rw-r--  2.0 unx        0 b- defN 23-Mar-31 21:33 torchx/examples/apps/datapreproc/__init__.py
 -rw-rw-r--  2.0 unx     4302 b- defN 23-Mar-31 21:33 torchx/examples/apps/datapreproc/datapreproc.py
 -rw-rw-r--  2.0 unx        0 b- defN 23-Mar-31 21:33 torchx/examples/apps/lightning/__init__.py
--rw-rw-r--  2.0 unx     6582 b- defN 23-Mar-31 21:33 torchx/examples/apps/lightning/data.py
+-rw-rw-r--  2.0 unx     6583 b- defN 23-Apr-10 16:06 torchx/examples/apps/lightning/data.py
 -rw-rw-r--  2.0 unx     5256 b- defN 23-Mar-31 21:33 torchx/examples/apps/lightning/interpret.py
 -rw-rw-r--  2.0 unx     3932 b- defN 23-Mar-31 21:33 torchx/examples/apps/lightning/model.py
 -rw-rw-r--  2.0 unx     1926 b- defN 23-Mar-31 21:33 torchx/examples/apps/lightning/profiler.py
 -rw-rw-r--  2.0 unx     6084 b- defN 23-Mar-31 21:33 torchx/examples/apps/lightning/train.py
 -rw-rw-r--  2.0 unx        0 b- defN 23-Mar-31 21:33 torchx/examples/pipelines/__init__.py
 -rw-rw-r--  2.0 unx        0 b- defN 23-Mar-31 21:33 torchx/examples/pipelines/kfp/__init__.py
 -rw-rw-r--  2.0 unx     8416 b- defN 23-Mar-31 21:33 torchx/examples/pipelines/kfp/advanced_pipeline.py
 -rw-rw-r--  2.0 unx     2178 b- defN 23-Mar-31 21:33 torchx/examples/pipelines/kfp/dist_pipeline.py
 -rw-rw-r--  2.0 unx     2751 b- defN 23-Mar-31 21:33 torchx/examples/pipelines/kfp/intro_pipeline.py
 -rw-rw-r--  2.0 unx      606 b- defN 23-Mar-31 21:33 torchx/pipelines/__init__.py
 -rw-rw-r--  2.0 unx      721 b- defN 23-Mar-31 21:33 torchx/pipelines/kfp/__init__.py
--rw-rw-r--  2.0 unx     8942 b- defN 23-Mar-31 21:33 torchx/pipelines/kfp/adapter.py
+-rw-rw-r--  2.0 unx     8952 b- defN 23-Apr-10 16:06 torchx/pipelines/kfp/adapter.py
 -rw-rw-r--  2.0 unx      524 b- defN 23-Mar-31 21:33 torchx/pipelines/kfp/version.py
 -rw-rw-r--  2.0 unx      300 b- defN 23-Mar-31 21:33 torchx/runner/__init__.py
--rw-rw-r--  2.0 unx    27108 b- defN 23-Mar-31 21:33 torchx/runner/api.py
--rw-rw-r--  2.0 unx    17013 b- defN 23-Mar-31 21:33 torchx/runner/config.py
+-rw-rw-r--  2.0 unx    26817 b- defN 23-Apr-10 16:06 torchx/runner/api.py
+-rw-rw-r--  2.0 unx    17171 b- defN 23-Apr-10 16:06 torchx/runner/config.py
 -rw-rw-r--  2.0 unx     3592 b- defN 23-Mar-31 21:33 torchx/runner/events/__init__.py
 -rw-rw-r--  2.0 unx     1957 b- defN 23-Mar-31 21:33 torchx/runner/events/api.py
 -rw-rw-r--  2.0 unx      507 b- defN 23-Mar-31 21:33 torchx/runner/events/handlers.py
 -rw-rw-r--  2.0 unx      593 b- defN 23-Mar-31 21:33 torchx/runtime/__init__.py
 -rw-rw-r--  2.0 unx     3040 b- defN 23-Mar-31 21:33 torchx/runtime/tracking/__init__.py
 -rw-rw-r--  2.0 unx     5457 b- defN 23-Mar-31 21:33 torchx/runtime/tracking/api.py
 -rw-rw-r--  2.0 unx     2157 b- defN 23-Mar-31 21:33 torchx/schedulers/__init__.py
 -rw-rw-r--  2.0 unx    13941 b- defN 23-Mar-31 21:33 torchx/schedulers/api.py
--rw-rw-r--  2.0 unx    21954 b- defN 23-Mar-31 21:33 torchx/schedulers/aws_batch_scheduler.py
+-rw-rw-r--  2.0 unx    26063 b- defN 23-Apr-10 16:06 torchx/schedulers/aws_batch_scheduler.py
 -rw-rw-r--  2.0 unx     1352 b- defN 23-Mar-31 21:33 torchx/schedulers/devices.py
--rw-rw-r--  2.0 unx    15247 b- defN 23-Mar-31 21:33 torchx/schedulers/docker_scheduler.py
+-rw-rw-r--  2.0 unx    15370 b- defN 23-Apr-10 16:06 torchx/schedulers/docker_scheduler.py
 -rw-rw-r--  2.0 unx    15993 b- defN 23-Mar-31 21:33 torchx/schedulers/gcp_batch_scheduler.py
 -rw-rw-r--  2.0 unx     1783 b- defN 23-Mar-31 21:33 torchx/schedulers/ids.py
--rw-rw-r--  2.0 unx    38388 b- defN 23-Mar-31 21:33 torchx/schedulers/kubernetes_mcad_scheduler.py
--rw-rw-r--  2.0 unx    26345 b- defN 23-Mar-31 21:33 torchx/schedulers/kubernetes_scheduler.py
--rw-rw-r--  2.0 unx    37442 b- defN 23-Mar-31 21:33 torchx/schedulers/local_scheduler.py
--rw-rw-r--  2.0 unx    17538 b- defN 23-Mar-31 21:33 torchx/schedulers/lsf_scheduler.py
--rw-rw-r--  2.0 unx    16235 b- defN 23-Apr-03 18:33 torchx/schedulers/ray_scheduler.py
+-rw-rw-r--  2.0 unx    41100 b- defN 23-Apr-10 16:06 torchx/schedulers/kubernetes_mcad_scheduler.py
+-rw-rw-r--  2.0 unx    26830 b- defN 23-Apr-10 16:06 torchx/schedulers/kubernetes_scheduler.py
+-rw-rw-r--  2.0 unx    37591 b- defN 23-Apr-10 16:06 torchx/schedulers/local_scheduler.py
+-rw-rw-r--  2.0 unx    17552 b- defN 23-Apr-10 16:06 torchx/schedulers/lsf_scheduler.py
+-rw-rw-r--  2.0 unx    16234 b- defN 23-Apr-10 16:06 torchx/schedulers/ray_scheduler.py
 -rw-rw-r--  2.0 unx    19269 b- defN 23-Mar-31 21:33 torchx/schedulers/slurm_scheduler.py
 -rw-rw-r--  2.0 unx     1992 b- defN 23-Mar-31 21:33 torchx/schedulers/streams.py
 -rw-rw-r--  2.0 unx      231 b- defN 23-Mar-31 21:33 torchx/schedulers/ray/__init__.py
 -rw-rw-r--  2.0 unx      610 b- defN 23-Mar-31 21:33 torchx/schedulers/ray/ray_common.py
 -rw-rw-r--  2.0 unx    12127 b- defN 23-Mar-31 21:33 torchx/schedulers/ray/ray_driver.py
--rw-rw-r--  2.0 unx     5369 b- defN 23-Mar-31 21:33 torchx/specs/__init__.py
+-rw-rw-r--  2.0 unx     5462 b- defN 23-Apr-10 16:06 torchx/specs/__init__.py
 -rw-rw-r--  2.0 unx    33798 b- defN 23-Mar-31 21:33 torchx/specs/api.py
 -rw-rw-r--  2.0 unx     8512 b- defN 23-Mar-31 21:33 torchx/specs/builders.py
 -rw-rw-r--  2.0 unx    11714 b- defN 23-Mar-31 21:33 torchx/specs/file_linter.py
--rw-rw-r--  2.0 unx    16128 b- defN 23-Mar-31 21:33 torchx/specs/finder.py
+-rw-rw-r--  2.0 unx    16234 b- defN 23-Apr-10 16:06 torchx/specs/finder.py
 -rw-rw-r--  2.0 unx     6361 b- defN 23-Mar-31 21:33 torchx/specs/named_resources_aws.py
+-rw-rw-r--  2.0 unx     2631 b- defN 23-Apr-10 16:06 torchx/specs/named_resources_generic.py
 -rw-rw-r--  2.0 unx      208 b- defN 23-Mar-31 21:33 torchx/specs/test/components/__init__.py
 -rw-rw-r--  2.0 unx      546 b- defN 23-Mar-31 21:33 torchx/specs/test/components/a/__init__.py
 -rw-rw-r--  2.0 unx      208 b- defN 23-Mar-31 21:33 torchx/specs/test/components/a/b/__init__.py
 -rw-rw-r--  2.0 unx      539 b- defN 23-Mar-31 21:33 torchx/specs/test/components/a/b/c.py
 -rw-rw-r--  2.0 unx      231 b- defN 23-Mar-31 21:33 torchx/specs/test/components/c/__init__.py
 -rw-rw-r--  2.0 unx      531 b- defN 23-Mar-31 21:33 torchx/specs/test/components/c/d.py
--rw-rw-r--  2.0 unx     4200 b- defN 23-Mar-31 21:33 torchx/tracker/__init__.py
--rw-rw-r--  2.0 unx    11299 b- defN 23-Mar-31 21:33 torchx/tracker/api.py
+-rw-rw-r--  2.0 unx     4206 b- defN 23-Apr-10 16:06 torchx/tracker/__init__.py
+-rw-rw-r--  2.0 unx    11263 b- defN 23-Apr-10 16:06 torchx/tracker/api.py
+-rw-rw-r--  2.0 unx    13221 b- defN 23-Apr-10 16:06 torchx/tracker/mlflow.py
 -rw-rw-r--  2.0 unx      231 b- defN 23-Mar-31 21:33 torchx/tracker/backend/__init__.py
 -rw-rw-r--  2.0 unx    10425 b- defN 23-Mar-31 21:33 torchx/tracker/backend/fsspec.py
 -rw-rw-r--  2.0 unx        0 b- defN 23-Mar-31 21:33 torchx/util/__init__.py
 -rw-rw-r--  2.0 unx      390 b- defN 23-Mar-31 21:33 torchx/util/datetime.py
 -rw-rw-r--  2.0 unx     2710 b- defN 23-Mar-31 21:33 torchx/util/entrypoints.py
 -rw-rw-r--  2.0 unx     1792 b- defN 23-Mar-31 21:33 torchx/util/io.py
 -rw-rw-r--  2.0 unx      448 b- defN 23-Mar-31 21:33 torchx/util/shlex.py
--rw-rw-r--  2.0 unx      660 b- defN 23-Mar-31 21:33 torchx/util/strings.py
+-rw-rw-r--  2.0 unx      663 b- defN 23-Apr-10 16:06 torchx/util/strings.py
 -rw-rw-r--  2.0 unx     7016 b- defN 23-Mar-31 21:33 torchx/util/types.py
 -rw-rw-r--  2.0 unx      783 b- defN 23-Mar-31 21:33 torchx/workspace/__init__.py
 -rw-rw-r--  2.0 unx     5464 b- defN 23-Mar-31 21:33 torchx/workspace/api.py
 -rw-rw-r--  2.0 unx     2253 b- defN 23-Mar-31 21:33 torchx/workspace/dir_workspace.py
 -rw-rw-r--  2.0 unx     9208 b- defN 23-Mar-31 21:33 torchx/workspace/docker_workspace.py
--rw-rw-r--  2.0 unx     1530 b- defN 23-Apr-03 18:34 codeflare_torchx-0.5.0.dev5.dist-info/LICENSE
--rw-rw-r--  2.0 unx     5327 b- defN 23-Apr-03 18:34 codeflare_torchx-0.5.0.dev5.dist-info/METADATA
--rw-rw-r--  2.0 unx       92 b- defN 23-Apr-03 18:34 codeflare_torchx-0.5.0.dev5.dist-info/WHEEL
--rw-rw-r--  2.0 unx      169 b- defN 23-Apr-03 18:34 codeflare_torchx-0.5.0.dev5.dist-info/entry_points.txt
--rw-rw-r--  2.0 unx        7 b- defN 23-Apr-03 18:34 codeflare_torchx-0.5.0.dev5.dist-info/top_level.txt
--rw-rw-r--  2.0 unx     9940 b- defN 23-Apr-03 18:34 codeflare_torchx-0.5.0.dev5.dist-info/RECORD
-113 files, 626349 bytes uncompressed, 210021 bytes compressed:  66.5%
+-rw-rw-r--  2.0 unx     1721 b- defN 23-Apr-10 16:08 codeflare_torchx-0.6.0.dev0.dist-info/LICENSE
+-rw-rw-r--  2.0 unx     5497 b- defN 23-Apr-10 16:08 codeflare_torchx-0.6.0.dev0.dist-info/METADATA
+-rw-rw-r--  2.0 unx       92 b- defN 23-Apr-10 16:08 codeflare_torchx-0.6.0.dev0.dist-info/WHEEL
+-rw-rw-r--  2.0 unx      169 b- defN 23-Apr-10 16:08 codeflare_torchx-0.6.0.dev0.dist-info/entry_points.txt
+-rw-rw-r--  2.0 unx        7 b- defN 23-Apr-10 16:08 codeflare_torchx-0.6.0.dev0.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx    10298 b- defN 23-Apr-10 16:08 codeflare_torchx-0.6.0.dev0.dist-info/RECORD
+117 files, 672687 bytes uncompressed, 224569 bytes compressed:  66.6%
```

## zipnote {}

```diff
@@ -84,14 +84,17 @@
 
 Filename: torchx/components/metrics.py
 Comment: 
 
 Filename: torchx/components/serve.py
 Comment: 
 
+Filename: torchx/components/structured_arg.py
+Comment: 
+
 Filename: torchx/components/train.py
 Comment: 
 
 Filename: torchx/components/utils.py
 Comment: 
 
 Filename: torchx/components/integration_tests/__init__.py
@@ -99,14 +102,17 @@
 
 Filename: torchx/components/integration_tests/component_provider.py
 Comment: 
 
 Filename: torchx/components/integration_tests/integ_tests.py
 Comment: 
 
+Filename: torchx/distributed/__init__.py
+Comment: 
+
 Filename: torchx/examples/__init__.py
 Comment: 
 
 Filename: torchx/examples/apps/__init__.py
 Comment: 
 
 Filename: torchx/examples/apps/datapreproc/__init__.py
@@ -252,14 +258,17 @@
 
 Filename: torchx/specs/finder.py
 Comment: 
 
 Filename: torchx/specs/named_resources_aws.py
 Comment: 
 
+Filename: torchx/specs/named_resources_generic.py
+Comment: 
+
 Filename: torchx/specs/test/components/__init__.py
 Comment: 
 
 Filename: torchx/specs/test/components/a/__init__.py
 Comment: 
 
 Filename: torchx/specs/test/components/a/b/__init__.py
@@ -276,14 +285,17 @@
 
 Filename: torchx/tracker/__init__.py
 Comment: 
 
 Filename: torchx/tracker/api.py
 Comment: 
 
+Filename: torchx/tracker/mlflow.py
+Comment: 
+
 Filename: torchx/tracker/backend/__init__.py
 Comment: 
 
 Filename: torchx/tracker/backend/fsspec.py
 Comment: 
 
 Filename: torchx/util/__init__.py
@@ -315,26 +327,26 @@
 
 Filename: torchx/workspace/dir_workspace.py
 Comment: 
 
 Filename: torchx/workspace/docker_workspace.py
 Comment: 
 
-Filename: codeflare_torchx-0.5.0.dev5.dist-info/LICENSE
+Filename: codeflare_torchx-0.6.0.dev0.dist-info/LICENSE
 Comment: 
 
-Filename: codeflare_torchx-0.5.0.dev5.dist-info/METADATA
+Filename: codeflare_torchx-0.6.0.dev0.dist-info/METADATA
 Comment: 
 
-Filename: codeflare_torchx-0.5.0.dev5.dist-info/WHEEL
+Filename: codeflare_torchx-0.6.0.dev0.dist-info/WHEEL
 Comment: 
 
-Filename: codeflare_torchx-0.5.0.dev5.dist-info/entry_points.txt
+Filename: codeflare_torchx-0.6.0.dev0.dist-info/entry_points.txt
 Comment: 
 
-Filename: codeflare_torchx-0.5.0.dev5.dist-info/top_level.txt
+Filename: codeflare_torchx-0.6.0.dev0.dist-info/top_level.txt
 Comment: 
 
-Filename: codeflare_torchx-0.5.0.dev5.dist-info/RECORD
+Filename: codeflare_torchx-0.6.0.dev0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## torchx/version.py

```diff
@@ -12,15 +12,16 @@
 #
 # Examples:
 # 0.1.0.devN # Developmental release
 # 0.1.0aN  # Alpha release
 # 0.1.0bN  # Beta release
 # 0.1.0rcN  # Release Candidate
 # 0.1.0  # Final release
-__version__ = "0.5.0dev5"
+__version__ = "0.6.0dev0"
+
 
 # Use the github container registry images corresponding to the current package
 # version.
 def _get_torchx_image(torchx_version: str) -> str:
     return f"ghcr.io/pytorch/torchx:{__version__}"
```

## torchx/cli/argparse_util.py

```diff
@@ -1,23 +1,19 @@
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 # All rights reserved.
 #
 # This source code is licensed under the BSD-style license found in the
 # LICENSE file in the root directory of this source tree.
 
 from argparse import Action, ArgumentParser, Namespace
-from pathlib import Path
 from typing import Any, Dict, Optional, Sequence, Text
 
 from torchx.runner import config
 
 
-CONFIG_DIRS = [str(Path.home()), str(Path.cwd())]
-
-
 class _torchxconfig(Action):
     """
     Custom argparse action that loads default torchx CLI options
     from .torchxconfig file.
 
     """
 
@@ -36,15 +32,14 @@
         **kwargs: Any,
     ) -> None:
         cfg = self._subcmd_configs.setdefault(
             subcmd,
             config.get_configs(
                 prefix="cli",
                 name=subcmd,
-                dirs=CONFIG_DIRS,
             ),
         )
 
         # if found in .torchxconfig make it the default for this argument
         # otherwise use the default defined from add_argument(...)
         default = cfg.get(dest, default)
```

## torchx/cli/cmd_configure.py

```diff
@@ -34,15 +34,14 @@
             "-a",
             "--all",
             action="store_true",
             help="if specified, includes required and optional runopts (default only dumps required)",
         )
 
     def run(self, args: argparse.Namespace) -> None:
-
         if args.schedulers:
             schedulers = args.schedulers.split(",")
         else:
             schedulers = get_scheduler_factories().keys()
 
         required_only = not args.all
```

## torchx/cli/cmd_run.py

```diff
@@ -11,15 +11,15 @@
 import threading
 from dataclasses import asdict
 from pathlib import Path
 from pprint import pformat
 from typing import Dict, List, Optional, Tuple
 
 import torchx.specs as specs
-from torchx.cli.argparse_util import CONFIG_DIRS, torchxconfig_run
+from torchx.cli.argparse_util import torchxconfig_run
 from torchx.cli.cmd_base import SubCommand
 from torchx.cli.cmd_log import get_logs
 from torchx.runner import config, get_runner, Runner
 from torchx.runner.config import load_sections
 from torchx.schedulers import get_default_scheduler_name, get_scheduler_factories
 from torchx.specs.finder import (
     _Component,
@@ -178,37 +178,36 @@
                 " removed in the near future,"
                 " please use other variants of the local scheduler"
                 " (e.g. `local_cwd`)"
             )
 
         scheduler_opts = runner.scheduler_run_opts(args.scheduler)
         cfg = scheduler_opts.cfg_from_str(args.scheduler_args)
-        config.apply(scheduler=args.scheduler, cfg=cfg, dirs=CONFIG_DIRS)
+        config.apply(scheduler=args.scheduler, cfg=cfg)
 
         component, component_args = _parse_component_name_and_args(
             args.component_name_and_args,
             none_throws(self._subparser),
-            dirs=CONFIG_DIRS,
         )
         try:
             if args.dryrun:
                 dryrun_info = runner.dryrun_component(
                     component,
                     component_args,
                     args.scheduler,
                     workspace=args.workspace,
                     cfg=cfg,
                     parent_run_id=args.parent_run_id,
                 )
-                logger.info(
+                print(
                     "\n=== APPLICATION ===\n"
                     f"{pformat(asdict(dryrun_info._app), indent=2, width=80)}"
                 )
 
-                logger.info("\n=== SCHEDULER REQUEST ===\n" f"{dryrun_info}")
+                print("\n=== SCHEDULER REQUEST ===\n" f"{dryrun_info}")
             else:
                 app_handle = runner.run_component(
                     component,
                     component_args,
                     args.scheduler,
                     workspace=args.workspace,
                     cfg=cfg,
@@ -239,15 +238,16 @@
                 "of setup `.torchxconfig` file, see: https://pytorch.org/torchx/main/experimental/runner.config.html"
             )
             logger.error(error_msg)
             sys.exit(1)
 
     def run(self, args: argparse.Namespace) -> None:
         os.environ["TORCHX_CONTEXT_NAME"] = os.getenv("TORCHX_CONTEXT_NAME", "cli_run")
-        component_defaults = load_sections(prefix="component", dirs=CONFIG_DIRS)
+        component_defaults = load_sections(prefix="component")
+
         with get_runner(component_defaults=component_defaults) as runner:
             self._run(runner, args)
 
     def _wait_and_exit(self, runner: Runner, app_handle: str, log: bool) -> None:
         logger.info("Waiting for the app to finish...")
 
         log_thread = self._start_log_thread(runner, app_handle) if log else None
```

## torchx/cli/cmd_status.py

```diff
@@ -48,14 +48,14 @@
     def run(self, args: argparse.Namespace) -> None:
         app_handle = args.app_handle
         scheduler, _, app_id = parse_app_handle(app_handle)
         runner = get_runner()
         app_status = runner.status(app_handle)
         filter_roles = parse_list_arg(args.roles)
         if app_status:
-            logger.info(app_status.format(filter_roles))
+            print(app_status.format(filter_roles))
         else:
             logger.error(
                 f"AppDef: {app_id},"
                 f" does not exist or has been removed from {scheduler}'s data plane"
             )
             sys.exit(1)
```

## torchx/cli/cmd_tracker.py

```diff
@@ -2,41 +2,24 @@
 # All rights reserved.
 #
 # This source code is licensed under the BSD-style license found in the
 # LICENSE file in the root directory of this source tree.
 
 import argparse
 import logging
-import sys
-from typing import Callable, Optional
 
 from tabulate import tabulate
 
 from torchx.cli.cmd_base import SubCommand
 from torchx.runner.api import get_configured_trackers
 from torchx.tracker.api import build_trackers, TrackerBase
-from torchx.util.types import none_throws
 
 logger: logging.Logger = logging.getLogger(__name__)
 
 
-def _requires_tracker(
-    command: Callable[["CmdTracker", argparse.Namespace], None]
-) -> Callable[["CmdTracker", argparse.Namespace], None]:
-    """Checks that command has valid tracker setup"""
-
-    def wrapper(self: "CmdTracker", args: argparse.Namespace) -> None:
-        if not self.tracker:
-            logger.error("Exiting since no trackers were configured.")
-            sys.exit(1)
-        command(self, args)
-
-    return wrapper
-
-
 class CmdTracker(SubCommand):
     """
     Prototype TorchX tracker subcommand that allows querying data by
     interacting with tracker implementation.
 
     Important: commands and the arguments may be modified in the future.
 
@@ -45,38 +28,36 @@
         - tracker list metadata RUN_ID
         - tracker list artifacts  [â€“artifact ARTIFACT_NAME] RUN_ID
     """
 
     def __init__(self) -> None:
         """
         Queries available tracker implementations and uses the first available one.
-
-        Since the instance needs to be available to setup torchx arguments, subcommands
-        utilize `_requires_tracker()` annotation to check that tracker is available
-        when invoked.
         """
-        self.tracker: Optional[TrackerBase] = None
-        configured_trackers = get_configured_trackers()
-        if configured_trackers:
-            trackers = build_trackers(configured_trackers)
-            if trackers:
-                self.tracker = next(iter(trackers))
-                logger.info(f"Using {self.tracker} to query data")
-            else:
-                logger.error("No trackers were configured!")
+
+    @property
+    def tracker(self) -> TrackerBase:
+        trackers = list(build_trackers(get_configured_trackers()))
+        if trackers:
+            logger.info(f"Using `{trackers[0]}` tracker to query data")
+            return trackers[0]
+        else:
+            raise RuntimeError(
+                "No trackers configured."
+                " See: https://pytorch.org/torchx/latest/runtime/tracking.html"
+            )
 
     def add_list_job_arguments(self, subparser: argparse.ArgumentParser) -> None:
         subparser.add_argument(
             "--parent-run-id", type=str, help="Optional job parent run ID"
         )
 
-    @_requires_tracker
     def list_jobs_command(self, args: argparse.Namespace) -> None:
         parent_run_id = args.parent_run_id
-        job_ids = none_throws(self.tracker).run_ids(parent_run_id=parent_run_id)
+        job_ids = self.tracker.run_ids(parent_run_id=parent_run_id)
 
         tabulated_job_ids = [[job_id] for job_id in job_ids]
         print(tabulate(tabulated_job_ids, headers=["JOB ID"]))
 
     def add_job_lineage_arguments(self, subparser: argparse.ArgumentParser) -> None:
         group = subparser.add_mutually_exclusive_group()
         group.add_argument(
@@ -87,51 +68,44 @@
         )
 
         subparser.add_argument(
             "--artifact", type=str, help="Limit to specific artifact"
         )
         subparser.add_argument("RUN_ID", type=str, help="Job run ID")
 
-    @_requires_tracker
     def job_lineage_command(self, args: argparse.Namespace) -> None:
         raise NotImplementedError("")
 
     def add_metadata_arguments(self, subparser: argparse.ArgumentParser) -> None:
         subparser.add_argument("RUN_ID", type=str, help="Job run ID")
 
-    @_requires_tracker
     def list_metadata_command(self, args: argparse.Namespace) -> None:
         run_id = args.RUN_ID
-        metadata = none_throws(self.tracker).metadata(run_id)
+        metadata = self.tracker.metadata(run_id)
         print_data = [[k, v] for k, v in metadata.items()]
 
         print(tabulate(print_data, headers=["ID", "VALUE"]))
 
     def add_artifacts_arguments(self, subparser: argparse.ArgumentParser) -> None:
         subparser.add_argument(
             "--artifact", type=str, help="Limit to specific artifact"
         )
 
         subparser.add_argument("RUN_ID", type=str, help="Job run ID")
 
-    @_requires_tracker
     def list_artifacts_command(self, args: argparse.Namespace) -> None:
         run_id = args.RUN_ID
         artifact_filter = args.artifact
 
-        artifacts = none_throws(self.tracker).artifacts(run_id)
-        artifacts = artifacts.values()
+        artifacts = list(self.tracker.artifacts(run_id).values())
 
         if artifact_filter:
-            artifacts = [
-                artifact for artifact in artifacts if artifact.name == artifact_filter
-            ]
-        print_data = [
-            [artifact.name, artifact.path, artifact.metadata] for artifact in artifacts
-        ]
+            artifacts = [a for a in artifacts if a.name == artifact_filter]
+
+        print_data = [[a.name, a.path, a.metadata] for a in artifacts]
 
         print(tabulate(print_data, headers=["ARTIFACT", "PATH", "METADATA"]))
 
     def add_arguments(self, subparser: argparse.ArgumentParser) -> None:
         tracker_subparsers = subparser.add_subparsers(
             description="Experimental tracker subcommands to query available tracked data.",
         )
```

## torchx/cli/main.py

```diff
@@ -1,14 +1,15 @@
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 # All rights reserved.
 #
 # This source code is licensed under the BSD-style license found in the
 # LICENSE file in the root directory of this source tree.
 
 import logging
+import os
 import sys
 from argparse import ArgumentParser
 from typing import Dict, List
 
 import torchx
 from torchx.cli.cmd_base import SubCommand
 from torchx.cli.cmd_cancel import CmdCancel
@@ -71,17 +72,17 @@
     """
     Helper function parsing the command line options.
     """
 
     parser = ArgumentParser(description="torchx CLI")
     parser.add_argument(
         "--log_level",
-        type=int,
+        type=str,
         help="Python logging log level",
-        default=logging.INFO,
+        default=os.getenv("LOGLEVEL", "INFO"),
     )
     parser.add_argument(
         "--version",
         action="version",
         version="torchx-{version}".format(version=torchx.version.__version__),
     )
     subparser = parser.add_subparsers(
```

## torchx/components/component_test_base.py

```diff
@@ -64,15 +64,18 @@
         .. code-block:: shell-session
 
          $ torchx run COMPONENT.py:FN --help
 
         """
 
         # make it the same as a custom component (e.g. /abs/path/to/component.py:train)
-        component_id = f"{os.path.abspath(module.__file__)}:{function_name}"
+        module_path = module.__file__
+        assert module_path, f"module must have __file__: {module_path}"
+        module_path = os.path.abspath(module_path)
+        component_id = f"{module_path}:{function_name}"
         component_def = get_component(component_id)
 
         # on `--help` argparse will print the help message and exit 0
         # just make sure that happens; if the component has errors then
         # this will raise an exception and the test will fail
         with self.assertRaises(SystemExit):
             _ = _create_args_parser(component_def.fn).parse_args(["--help"])
```

## torchx/components/dist.py

```diff
@@ -58,14 +58,15 @@
 import re
 import shlex
 from pathlib import Path
 from typing import Dict, Iterable, List, Optional, Tuple
 
 import torchx
 import torchx.specs as specs
+from torchx.components.structured_arg import StructuredJArgument, StructuredNameArgument
 from torchx.specs import macros
 
 _TORCH_DEBUG_FLAGS: Dict[str, str] = {
     "CUDA_LAUNCH_BLOCKING": "1",
     "NCCL_DESYNC_DEBUG": "1",
     "TORCH_DISTRIBUTED_DEBUG": "DETAIL",
     "TORCH_SHOW_CPP_STACKTRACES": "1",
@@ -76,28 +77,105 @@
 * ``CUDA_LAUNCH_BLOCKING``: Read more `here <https://docs.nvidia.com/cuda/cuda-gdb/index.html#set-cuda-launch-blocking>`__.
 * ``NCCL_DESYNC_DEBUG``
 * ``TORCH_DISTRIBUTED_DEBUG``: Read more `here <https://pytorch.org/docs/stable/distributed.html#torch-distributed-debug>`__.
 * ``TORCH_SHOW_CPP_STACKTRACES``: Read more `here <https://pytorch.org/docs/stable/distributed.html#torch-distributed-debug>`__.
 """
 
 
+def spmd(
+    *args: str,
+    script: Optional[str] = None,
+    m: Optional[str] = None,
+    image: str = torchx.IMAGE,
+    name: str = "/",
+    h: str = "gpu.small",
+    j: str = "1x1",
+    env: Optional[Dict[str, str]] = None,
+    max_retries: int = 0,
+    mounts: Optional[List[str]] = None,
+    debug: bool = False,
+) -> specs.AppDef:
+    """
+    Usage (by script): torchx run spmd -j 2x8 -h aws_p4d.24xlarge --name my_experiment/trial_1 --script path/to/my/trainer.py -foo bar
+
+    Usage (by module): torchx run spmd -j 2x8 -h aws_p4d.24xlarge --name my_experiment/trial_1 -m path.to.my.trainer -foo bar
+
+    Usage (infer GPU count): torchx run spmd -j 2 -h p4d.24xlarge ... (same as -j 2x8)
+
+    Creates a torchx.specs.AppDef (Job Definition) for a Single-Process-Multiple-Data (SPMD)
+    style application. See: https://en.wikipedia.org/wiki/Single_program,_multiple_data.
+
+    SPMD launches `n x m` (set via the `-j nxm` option) copies of the same program,
+    where `n` is the number of nodes (hosts) and `m` is the number of processes on each node.
+
+    If you have a distributed PyTorch script (DDP, FSDP, RPC) use this component to launch
+    the distributed application. You can also use `-j 1x1` to launch a single process application
+    which would be equivalent to launching with regular `python` except that your application
+    can safely call `torch.distributed.init_process_group(backend)`.
+
+    Note: For multi-node distributed runs, the hosts MUST have a network route to each other
+          AND port 29500 should be open on all hosts. Please check your security group settings.
+
+
+    Args:
+        args: the arguments to the main module or script (e.g. my/trainer.py -foo bar)
+            (for docker based runs) the script path must be relative to the WORKDIR of the image
+        script:
+        m: the main module name (e.g. my.module.trainer). When this option is used, the `script_args` are passed
+           as the arguments to the main module). Invoking my module is useful when the relative/absolute path
+           of the main script is unknown w.r.t the WORKDIR of the image. Use this option when it makes sense to
+           invoke the main script via `python -m <MAIN.MODULE>`.
+        image: the base docker image of the workspace, if workspace is disabled, then the image of the job
+        name: ``{experimentname}/{runname}`` or ``{experimentname}/`` or ``/{runname}`` or ``{runname}``
+        h: the type of host to run on (e.g. aws_p4d.24xlarge). Must be one of the registered named resources
+        j: {nnodes}x{nproc_per_node}. For GPU hosts omitting nproc_per_node will infer it from the GPU count on the host
+        env: environment variables to be passed to the run (e.g. ENV1=v1,ENV2=v2,ENV3=v3)
+        max_retries: the number of scheduler retries allowed
+        rdzv_port: the port on rank0's host to use for hosting the c10d store used for rendezvous.
+                   Only takes effect when running multi-node. When running single node, this parameter
+                   is ignored and a random free port is chosen.
+        mounts: (for docker based runs only) mounts to mount into the worker environment/container
+                (ex. type=<bind/volume>,src=/host,dst=/job[,readonly]).
+        debug: whether to run with preset debug flags enabled
+
+    """
+
+    if env is None:
+        env = {}
+
+    return ddp(
+        *args,
+        script=script,
+        m=m,
+        image=image,
+        name=name,
+        h=h,
+        j=str(StructuredJArgument.parse_from(h, j)),
+        env=env,
+        max_retries=max_retries,
+        mounts=mounts,
+        debug=debug,
+    )
+
+
 def ddp(
     *script_args: str,
     script: Optional[str] = None,
     m: Optional[str] = None,
     image: str = torchx.IMAGE,
-    name: Optional[str] = None,
+    name: str = "/",
     h: Optional[str] = None,
     cpu: int = 2,
     gpu: int = 0,
     memMB: int = 1024,
     j: str = "1x2",
     env: Optional[Dict[str, str]] = None,
     max_retries: int = 0,
     rdzv_port: int = 29500,
+    rdzv_backend: str = "c10d",
     mounts: Optional[List[str]] = None,
     debug: bool = False,
 ) -> specs.AppDef:
     """
     Distributed data parallel style application (one role, multi-replica).
     Uses `torch.distributed.run <https://pytorch.org/docs/stable/distributed.elastic.html>`_
     to launch and coordinate PyTorch worker processes. Defaults to using ``c10d`` rendezvous backend
@@ -110,15 +188,16 @@
           See `registering named resources <https://pytorch.org/torchx/latest/advanced.html#registering-named-resources>`_.
 
     Args:
         script_args: arguments to the main module
         script: script or binary to run within the image
         m: the python module path to run
         image: image (e.g. docker)
-        name: job name override (uses the script name if not specified)
+        name: job name override in the following format: ``{experimentname}/{runname}`` or ``{experimentname}/`` or ``/{runname}`` or ``{runname}``.
+            Uses the script or module name if ``{runname}`` not specified.
         cpu: number of cpus per replica
         gpu: number of gpus per replica
         memMB: cpu memory in MB per replica
         h: a registered named resource (if specified takes precedence over cpu, gpu, memMB)
         j: [{min_nnodes}:]{nnodes}x{nproc_per_node}, for gpu hosts, nproc_per_node must not exceed num gpus
         env: environment varibles to be passed to the run (e.g. ENV1=v1,ENV2=v2,ENV3=v3)
         max_retries: the number of scheduler retries allowed
@@ -134,44 +213,51 @@
         raise ValueError("exactly one of --script and -m must be specified")
 
     # nnodes: number of nodes or minimum nodes for elastic launch
     # max_nnodes: maximum number of nodes for elastic launch
     # nproc_per_node: number of processes on each node
     min_nnodes, max_nnodes, nproc_per_node, nnodes_rep = parse_nnodes(j)
 
-    if script:
-        # script name/module no extension
-        role_name = Path(script).stem
-    elif m:
-        role_name = m.rpartition(".")[2]
-    else:
-        raise ValueError("failed to compute role_name")
-
-    rdzv_backend = "static"
+  
     if max_nnodes == 1:
         # using port 0 makes elastic chose a free random port which is ok
         # for single-node jobs since all workers run under a single agent
         # When nnodes is 0 and max_nnodes is 1, it's stil a single node job
         # but pending until the resources become available
         rdzv_endpoint = _noquote(f"$${macros.rank0_env}:49782")
     else:
-        rdzv_endpoint = _noquote(f"$${macros.rank0_env}:49782")
+        # for multi-node, rely on the rank0_env environment variable set by
+        # the schedulers (see scheduler implementation for the actual env var this maps to)
+        # some schedulers (e.g. aws batch) make the rank0's ip-addr available on all BUT on rank0
+        # so default to "localhost" if the env var is not set or is empty
+        # rdzv_endpoint bash resolves to something to the effect of
+        # ${TORCHX_RANK0_HOST:=localhost}:29500
+        # use $$ in the prefix to escape the '$' literal (rather than a string Template substitution argument)
+        if rdzv_backend == "static":
+            rdzv_endpoint = _noquote(f"$${macros.rank0_env}:49782")
+        else:
+            rdzv_endpoint = _noquote(f"$${{{macros.rank0_env}:=localhost}}:{rdzv_port}")
 
     if env is None:
         env = {}
-    env.setdefault("LOGLEVEL", os.getenv("LOGLEVEL", "DEBUG"))
-    env.setdefault("TORCH_DISTRIBUTED_DEBUG", "DETAIL")
 
+    argname = StructuredNameArgument.parse_from(
+        name=name,
+        m=m,
+        script=script,
+    )
+
+    env["TORCHX_TRACKING_EXPERIMENT_NAME"] = argname.experiment_name
+
+    env.setdefault("LOGLEVEL", os.getenv("LOGLEVEL", "WARNING"))
     if debug:
         env.update(_TORCH_DEBUG_FLAGS)
 
     cmd = [
-        "python",
-        "-m",
-        "torch.distributed.run",
+        "torchrun",
         "--rdzv_backend",
         rdzv_backend,
         "--rdzv_endpoint",
         rdzv_endpoint,
         "--rdzv_id",
         f"{macros.app_id}",
         "--nnodes",
@@ -187,18 +273,18 @@
     ]
     if script is not None:
         cmd += [script]
     elif m is not None:
         cmd += ["-m", m]
     cmd += script_args
     return specs.AppDef(
-        name=name or role_name,
+        name=argname.run_name,
         roles=[
             specs.Role(
-                name=role_name,
+                name=get_role_name(script, m),
                 image=image,
                 min_replicas=min_nnodes,
                 entrypoint="bash",
                 num_replicas=int(max_nnodes),
                 resource=specs.resource(cpu=cpu, gpu=gpu, memMB=memMB, h=h),
                 args=["-c", _args_join(cmd)],
                 env=env,
@@ -208,14 +294,25 @@
                 max_retries=max_retries,
                 mounts=specs.parse_mounts(mounts) if mounts else [],
             )
         ],
     )
 
 
+def get_role_name(script: Optional[str], m: Optional[str]) -> str:
+    if script:
+        # script name/module no extension
+        role_name = Path(script).stem
+    elif m:
+        role_name = m.rpartition(".")[2]
+    else:
+        raise ValueError("failed to compute role_name")
+    return role_name
+
+
 def _args_join(args: Iterable[str]) -> str:
     """
     _args_join is like shlex.join but if the argument is wrapped in _noquote
     it'll not quote that argument.
     """
     quoted = [arg if isinstance(arg, _noquote) else shlex.quote(arg) for arg in args]
     return " ".join(quoted)
```

## torchx/components/integration_tests/component_provider.py

```diff
@@ -33,20 +33,23 @@
     def tearDown(self) -> None:
         pass
 
 
 class DDPComponentProvider(ComponentProvider):
     def get_app_def(self) -> AppDef:
         return dist_components.ddp(
-            script="torchx/components/integration_tests/test/dummy_app.py",
+            m="torchx.examples.apps.compute_world_size.main",
             name="ddp-trainer",
             image=self._image,
             cpu=1,
             j="2x2",
             max_retries=3,
+            env={
+                "LOGLEVEL": "INFO",
+            },
         )
 
 
 class ServeComponentProvider(ComponentProvider):
     # TODO(aivanou): Remove dryrun and test e2e serve component+app
     def get_app_def(self) -> AppDef:
         return serve_components.torchserve(
```

## torchx/examples/apps/lightning/data.py

```diff
@@ -57,14 +57,15 @@
         return super().__len__()
 
 
 # %%
 # For easy of use, we define a lightning data module so we can reuse it across
 # our trainer and other components that need to load data.
 
+
 # pyre-fixme[13]: Attribute `test_ds` is never initialized.
 # pyre-fixme[13]: Attribute `train_ds` is never initialized.
 # pyre-fixme[13]: Attribute `val_ds` is never initialized.
 class TinyImageNetDataModule(pl.LightningDataModule):
     """
     TinyImageNetDataModule is a pytorch LightningDataModule for the tiny
     imagenet dataset.
```

## torchx/pipelines/kfp/adapter.py

```diff
@@ -171,15 +171,15 @@
             container.add_port(
                 V1ContainerPort(
                     name=name,
                     container_port=port,
                 ),
             )
 
-        c.pod_labels.update(pod_labels(app, 0, role_spec, 0))
+        c.pod_labels.update(pod_labels(app, 0, role_spec, 0, app.name))
 
         return c
 
     return factory_wrapper
 
 
 def _ui_metadata_sidecar(
```

## torchx/runner/api.py

```diff
@@ -27,55 +27,51 @@
     make_app_handle,
     materialize_appdef,
     parse_app_handle,
     runopts,
     UnknownAppException,
 )
 from torchx.specs.finder import get_component
-from torchx.tracker.api import tracker_config_env_var_name, TRACKER_ENV_VAR_NAME
+from torchx.tracker.api import (
+    ENV_TORCHX_JOB_ID,
+    ENV_TORCHX_PARENT_RUN_ID,
+    ENV_TORCHX_TRACKERS,
+    tracker_config_env_var_name,
+)
 
 from torchx.util.types import none_throws
 from torchx.workspace.api import WorkspaceMixin
 
 from .config import get_config, get_configs
 
 logger: logging.Logger = logging.getLogger(__name__)
 
 
 NONE: str = "<NONE>"
 
 
 def get_configured_trackers() -> Dict[str, Optional[str]]:
-    tracker_names = []
-    if TRACKER_ENV_VAR_NAME in os.environ and os.environ[TRACKER_ENV_VAR_NAME]:
-        trackers = os.environ[TRACKER_ENV_VAR_NAME]
-        tracker_names = os.environ[TRACKER_ENV_VAR_NAME].split(",")
-        logger.info(
-            f"Using 'TORCHX_TRACKERS'='{trackers}' env variable to setup trackers"
-        )
-    else:
-        tracker_names = get_configs(prefix="torchx", name="tracker")
-        if tracker_names:
-            names = (",").join(tracker_names.keys())
-            logger.info(f"Using #torchx.tracker=['{names}'] config to setup trackers")
+    tracker_names = list(get_configs(prefix="torchx", name="tracker").keys())
+    if ENV_TORCHX_TRACKERS in os.environ:
+        logger.info(f"Using TORCHX_TRACKERS={tracker_names} as tracker names")
+        tracker_names = os.environ[ENV_TORCHX_TRACKERS].split(",")
 
     tracker_names_with_config = {}
     for tracker_name in tracker_names:
-        config_env_name = tracker_config_env_var_name(tracker_name)
-        config_value = None
+        config_value = get_config(prefix="tracker", name=tracker_name, key="config")
 
+        config_env_name = tracker_config_env_var_name(tracker_name)
         if config_env_name in os.environ:
             config_value = os.environ[config_env_name]
             logger.info(
-                f"Using {config_env_name}=['{config_value}'] config to setup {tracker_name} tracker"
+                f"Using {config_env_name}={config_value} for `{tracker_name}` tracker"
             )
-        else:
-            config_value = get_config(prefix="tracker", name=tracker_name, key="config")
 
         tracker_names_with_config[tracker_name] = config_value
+    logger.info(f"Tracker configurations: {tracker_names_with_config}")
     return tracker_names_with_config
 
 
 class Runner:
     """
     TorchX individual component runner. Has the methods for the user to
     act upon ``AppDefs``. The ``Runner`` will cache information about the
@@ -314,18 +310,18 @@
         """
         # input validation
         if not app.roles:
             raise ValueError(
                 f"No roles for app: {app.name}. Did you forget to add roles to AppDef?"
             )
 
-        if "TORCHX_PARENT_RUN_ID" in os.environ:
-            parent_run_id = os.environ["TORCHX_PARENT_RUN_ID"]
+        if ENV_TORCHX_PARENT_RUN_ID in os.environ:
+            parent_run_id = os.environ[ENV_TORCHX_PARENT_RUN_ID]
             logger.info(
-                f"Using 'TORCHX_PARENT_RUN_ID'='{parent_run_id}' env variable as tracker parent run ID"
+                f"Using {ENV_TORCHX_PARENT_RUN_ID}={parent_run_id} env variable as tracker parent run id"
             )
 
         configured_trackers = get_configured_trackers()
 
         for role in app.roles:
             if not role.entrypoint:
                 raise ValueError(
@@ -340,24 +336,23 @@
             # Setup tracking
             # 1. Inject parent identifier
             # 2. Inject this run's job ID
             # 3. Get the list of backends to support from .torchconfig
             #    - inject it as TORCHX_TRACKERS=names (it is expected that entrypoints are defined)
             #    - for each backend check configuration file, if exists:
             #        - inject it as TORCHX_TRACKER_<name>_CONFIGFILE=filename
-            role.env["TORCHX_JOB_ID"] = make_app_handle(
+            role.env[ENV_TORCHX_JOB_ID] = make_app_handle(
                 scheduler, self._name, macros.app_id
             )
 
-            # TODO extract constants into tracker API module
             if parent_run_id:
-                role.env["TORCHX_PARENT_RUN_ID"] = parent_run_id
+                role.env[ENV_TORCHX_PARENT_RUN_ID] = parent_run_id
 
             if configured_trackers:
-                role.env[TRACKER_ENV_VAR_NAME] = ",".join(configured_trackers.keys())
+                role.env[ENV_TORCHX_TRACKERS] = ",".join(configured_trackers.keys())
 
             for name, config in configured_trackers.items():
                 if config:
                     role.env[tracker_config_env_var_name(name)] = config
 
         cfg = cfg or dict()
         with log_event("dryrun", scheduler, runcfg=json.dumps(cfg) if cfg else None):
```

## torchx/runner/config.py

```diff
@@ -165,14 +165,15 @@
 from torchx.specs import CfgVal, get_type_name
 from torchx.specs.api import runopt
 
 
 CONFIG_FILE = ".torchxconfig"
 CONFIG_PREFIX_DELIM = ":"
 ENV_TORCHXCONFIG = "TORCHXCONFIG"
+DEFAULT_CONFIG_DIRS = [str(Path.home()), str(Path.cwd())]
 
 _NONE = "None"
 
 log: logging.Logger = logging.getLogger(__name__)
 
 
 def _configparser() -> configparser.ConfigParser:
@@ -386,14 +387,17 @@
             config.read_file(f)
         for section_name in config.sections():
             name = strip_prefix(section_name)
             if name:
                 section = sections.setdefault(name, {})
                 for key, value in config.items(section_name):
                     if key not in section:
+                        log.debug(
+                            f"Loaded config: {prefix}.{key}={value} from {configfile}"
+                        )
                         section[key] = value
 
     return sections
 
 
 def get_configs(
     prefix: str,
@@ -453,31 +457,31 @@
     on the following logic:
 
     1. If the environment variable ``TORCHXCONFIG`` exists, then its value
        is returned in a single-element list and the directories specified through
        the ``dirs`` parameter is NOT searched.
     2. Otherwise, a ``.torchxconfig`` file is looked for in ``dirs`` and
        the filepaths to existing config files are returned. If ``dirs`` is
-       not specified or is empty then, this method looks for a ``.torchxconfig`` file
-       in CWD (current working dir) and returns the filepath to it if one exists.
+       not specified or is empty then ``dirs`` defaults to ``[$HOME, $CWD]``
+       where CWD is current working dir.
 
     """
 
     config = os.getenv(ENV_TORCHXCONFIG)
     if config is not None:
         configfile = Path(config)
         if not configfile.is_file():
             raise FileNotFoundError(
                 f"`{ENV_TORCHXCONFIG}={config}` does not exist or is not a file."
             )
         return [str(configfile)]
     else:
         config_files = []
         if not dirs:
-            dirs = [str(Path.cwd())]
+            dirs = DEFAULT_CONFIG_DIRS
         for d in dirs:
             configfile = Path(d) / CONFIG_FILE
             if configfile.exists():
                 config_files.append(str(configfile))
     return config_files
```

## torchx/schedulers/aws_batch_scheduler.py

```diff
@@ -33,16 +33,17 @@
 See
 https://docs.aws.amazon.com/AmazonECR/latest/userguide/getting-started-cli.html#cli-create-repository
 for how to create a image repository.
 """
 import getpass
 import re
 import threading
-from dataclasses import dataclass
+from dataclasses import asdict, dataclass
 from datetime import datetime
+from enum import auto, Enum
 from typing import (
     Any,
     Callable,
     cast,
     Dict,
     Iterable,
     List,
@@ -59,30 +60,41 @@
     AppDryRunInfo,
     DescribeAppResponse,
     filter_regex,
     ListAppResponse,
     Scheduler,
     Stream,
 )
+
 from torchx.schedulers.devices import get_device_mounts
 from torchx.schedulers.ids import make_unique
 from torchx.specs.api import (
     AppDef,
     AppState,
     BindMount,
     CfgVal,
     DeviceMount,
+    is_terminal,
     macros,
+    MISSING,
+    Resource,
     Role,
     runopts,
     VolumeMount,
 )
+from torchx.util.types import none_throws
 from torchx.workspace.docker_workspace import DockerWorkspaceMixin
 from typing_extensions import TypedDict
 
+ENV_TORCHX_ROLE_IDX = "TORCHX_ROLE_IDX"
+
+ENV_TORCHX_ROLE_NAME = "TORCHX_ROLE_NAME"
+
+DEFAULT_ROLE_NAME = "node"
+
 TAG_TORCHX_VER = "torchx.pytorch.org/version"
 TAG_TORCHX_APPNAME = "torchx.pytorch.org/app-name"
 TAG_TORCHX_USER = "torchx.pytorch.org/user"
 
 
 if TYPE_CHECKING:
     from docker import DockerClient
@@ -94,33 +106,76 @@
     "STARTING": AppState.PENDING,
     "RUNNING": AppState.RUNNING,
     "SUCCEEDED": AppState.SUCCEEDED,
     "FAILED": AppState.FAILED,
 }
 
 
-def _role_to_node_properties(idx: int, role: Role) -> Dict[str, object]:
-    resource = role.resource
-    reqs = []
-    cpu = resource.cpu
-    if cpu <= 0:
-        cpu = 1
-    reqs.append({"type": "VCPU", "value": str(cpu)})
+def to_millis_since_epoch(ts: datetime) -> int:
+    # datetime's timestamp returns seconds since epoch
+    return int(round(ts.timestamp() * 1000))
 
-    memMB = resource.memMB
-    if memMB < 0:
+
+def to_datetime(ms_since_epoch: int) -> datetime:
+    return datetime.fromtimestamp(ms_since_epoch / 1000)
+
+
+class ResourceType(Enum):
+    VCPU = auto()
+    GPU = auto()
+    MEMORY = auto()
+
+    @staticmethod
+    def from_str(resource_type: str) -> "ResourceType":
+        for rt in ResourceType:
+            if rt.name == resource_type.upper():
+                return rt
         raise ValueError(
-            f"AWSBatchScheduler requires memMB to be set to a positive value, got {memMB}"
+            f"No ResourceType found for `{resource_type}`. Valid types: {[r.name for r in ResourceType]}"
         )
-    reqs.append({"type": "MEMORY", "value": str(memMB)})
 
-    if resource.gpu > 0:
-        reqs.append({"type": "GPU", "value": str(resource.gpu)})
 
-    role.mounts += get_device_mounts(resource.devices)
+def resource_requirements_from_resource(resource: Resource) -> List[Dict[str, str]]:
+    cpu = resource.cpu if resource.cpu > 0 else 1
+    gpu = resource.gpu
+    memMB = resource.memMB
+    assert (
+        memMB > 0
+    ), f"AWSBatchScheduler requires memMB to be set to a positive value, got {memMB}"
+
+    resource_requirements = [
+        {"type": ResourceType.VCPU.name, "value": str(cpu)},
+        {"type": ResourceType.MEMORY.name, "value": str(memMB)},
+    ]
+    if gpu > 0:
+        resource_requirements.append({"type": ResourceType.GPU.name, "value": str(gpu)})
+    return resource_requirements
+
+
+def resource_from_resource_requirements(
+    resource_requirements: List[Dict[str, str]]
+) -> Resource:
+    resrc_req = {
+        ResourceType.from_str(r["type"]): int(r["value"]) for r in resource_requirements
+    }
+    return Resource(
+        cpu=resrc_req[ResourceType.VCPU],
+        gpu=resrc_req.get(ResourceType.GPU, 0),
+        memMB=resrc_req[ResourceType.MEMORY],
+        # TODO kiukchung@ map back capabilities and devices
+        # might be better to tag the named resource and finding the resource
+        # this requires the named resource to be part of the AppDef spec
+        # but today we lose the named resource str at the component level
+    )
+
+
+def _role_to_node_properties(
+    role: Role, start_idx: int, privileged: bool = False
+) -> Dict[str, object]:
+    role.mounts += get_device_mounts(role.resource.devices)
 
     mount_points = []
     volumes = []
     devices = []
     for i, mount in enumerate(role.mounts):
         name = f"mount_{i}"
         if isinstance(mount, BindMount):
@@ -171,30 +226,31 @@
         else:
             raise TypeError(f"unknown mount type {mount}")
 
     container = {
         "command": [role.entrypoint] + role.args,
         "image": role.image,
         "environment": [{"name": k, "value": v} for k, v in role.env.items()],
-        "resourceRequirements": reqs,
+        "privileged": privileged,
+        "resourceRequirements": resource_requirements_from_resource(role.resource),
         "linuxParameters": {
             # To support PyTorch dataloaders we need to set /dev/shm to larger
             # than the 64M default.
-            "sharedMemorySize": memMB,
+            "sharedMemorySize": role.resource.memMB,
             "devices": devices,
         },
         "logConfiguration": {
             "logDriver": "awslogs",
         },
         "mountPoints": mount_points,
         "volumes": volumes,
     }
 
     return {
-        "targetNodes": str(idx),
+        "targetNodes": f"{start_idx}:{start_idx + role.num_replicas - 1}",
         "container": container,
     }
 
 
 def _job_ui_url(job_arn: str) -> Optional[str]:
     match = re.match(
         "arn:aws:batch:([a-z-0-9]+):[0-9]+:job/([a-z-0-9]+)",
@@ -203,24 +259,58 @@
     if match is None:
         return None
     region = match.group(1)
     job_id = match.group(2)
     return f"https://{region}.console.aws.amazon.com/batch/home?region={region}#jobs/mnp-job/{job_id}"
 
 
+def _parse_num_replicas(target_nodes: str, num_nodes: int) -> int:
+    """
+    Parses the number of replicas for a role given the target_nodes string
+    and total num_nodes. See docstring for ``_parse_start_and_end_idx()``
+    for details on the format of ``target_nodes`` string.
+    """
+
+    start_idx, end_idx = _parse_start_and_end_idx(target_nodes, num_nodes)
+    return end_idx - start_idx + 1
+
+
+def _parse_start_and_end_idx(target_nodes: str, num_nodes: int) -> Tuple[int, int]:
+    """
+    Takes the ``target_nodes`` str (as required by AWS Batch NodeRangeProperties)
+    and parses out the start and end indices (aka global rank) of the replicas in the node group.
+    The ``target_nodes`` string is of the form:
+
+    #. ``[start_node_index]:[end_node_index]`` (e.g. ``0:5``)
+    #. --or-- ``:[end_node_index]`` (e.g. ``:5``)
+    #. --or-- ``[start_node_index]:`` (e.g. ``0:``)
+    #. --or-- ``[node_index]`` (e.g. ``0`` - single node multi-node-parallel job)
+
+    See: https://docs.aws.amazon.com/batch/latest/userguide/job_definition_parameters.html
+    """
+
+    indices = target_nodes.split(":")
+    if len(indices) == 1:
+        return int(indices[0]), int(indices[0])
+    else:
+        start_idx = indices[0]
+        end_idx = indices[1]
+        return int(start_idx or "0"), int(end_idx or str(num_nodes - 1))
+
+
 @dataclass
 class BatchJob:
     name: str
     queue: str
     share_id: Optional[str]
     job_def: Dict[str, object]
     images_to_push: Dict[str, Tuple[str, str]]
 
     def __str__(self) -> str:
-        return yaml.dump(self.job_def)
+        return yaml.dump(asdict(self))
 
     def __repr__(self) -> str:
         return str(self)
 
 
 T = TypeVar("T")
 
@@ -247,16 +337,17 @@
     return boto3.session.Session()
 
 
 class AWSBatchOpts(TypedDict, total=False):
     queue: str
     user: str
     image_repo: Optional[str]
+    privileged: bool
     share_id: Optional[str]
-    priority: Optional[int]
+    priority: int
 
 
 class AWSBatchScheduler(DockerWorkspaceMixin, Scheduler[AWSBatchOpts]):
     """
     AWSBatchScheduler is a TorchX scheduling interface to AWS Batch.
 
     .. code-block:: bash
@@ -363,60 +454,58 @@
 
     def _submit_dryrun(self, app: AppDef, cfg: AWSBatchOpts) -> AppDryRunInfo[BatchJob]:
         queue = cfg.get("queue")
         if not isinstance(queue, str):
             raise TypeError(f"config value 'queue' must be a string, got {queue}")
 
         share_id = cfg.get("share_id")
-        priority = cfg.get("priority")
-        if share_id is None and priority is not None:
-            raise ValueError(
-                "config value 'priority' takes no effect for job queues without a scheduling policy "
-                "(implied by 'share_id' not being set)"
-            )
+        priority = cfg["priority"]
 
         name_suffix = f"-{share_id}" if share_id is not None else ""
         name = make_unique(f"{app.name}{name_suffix}")
 
+        assert len(app.roles) <= 5, (
+            "AWS Batch only supports <= 5 roles (NodeGroups)."
+            " See: https://docs.aws.amazon.com/batch/latest/userguide/multi-node-parallel-jobs.html#mnp-node-groups"
+        )
+
         # map any local images to the remote image
         images_to_push = self.dryrun_push_images(app, cast(Mapping[str, CfgVal], cfg))
 
         nodes = []
-
+        node_idx = 0
         for role_idx, role in enumerate(app.roles):
-            for replica_id in range(role.num_replicas):
-                rank = len(nodes)
-                values = macros.Values(
-                    img_root="",
-                    app_id=name,
-                    replica_id=str(replica_id),
-                    rank0_env=(
-                        "TORCHX_RANK0_HOST"
-                        if rank == 0
-                        else "AWS_BATCH_JOB_MAIN_NODE_PRIVATE_IPV4_ADDRESS"
-                    ),
+            values = macros.Values(
+                img_root="",
+                app_id=name,
+                # this only resolves for role.args
+                # if the entrypoint is run with sh or bash
+                # but won't actually work for macros in env vars
+                replica_id="$AWS_BATCH_JOB_NODE_INDEX",
+                rank0_env="AWS_BATCH_JOB_MAIN_NODE_PRIVATE_IPV4_ADDRESS",
+            )
+            role = values.apply(role)
+            role.env[ENV_TORCHX_ROLE_IDX] = str(role_idx)
+            role.env[ENV_TORCHX_ROLE_NAME] = str(role.name)
+
+            nodes.append(
+                _role_to_node_properties(
+                    role,
+                    start_idx=node_idx,
+                    privileged=cfg["privileged"],
                 )
-                replica_role = values.apply(role)
-                replica_role.env["TORCHX_ROLE_IDX"] = str(role_idx)
-                replica_role.env["TORCHX_ROLE_NAME"] = str(role.name)
-                replica_role.env["TORCHX_REPLICA_IDX"] = str(replica_id)
-                if rank == 0:
-                    # AWS_BATCH_JOB_MAIN_NODE_PRIVATE_IPV4_ADDRESS is only
-                    # available on the child workers so we set the address to
-                    # localhost for rank0.
-                    # See: https://docs.aws.amazon.com/batch/latest/userguide/job_env_vars.html
-                    replica_role.env["TORCHX_RANK0_HOST"] = "localhost"
-                nodes.append(_role_to_node_properties(rank, replica_role))
+            )
+            node_idx += role.num_replicas
 
         job_def = {
             **{
                 "jobDefinitionName": name,
                 "type": "multinode",
                 "nodeProperties": {
-                    "numNodes": len(nodes),
+                    "numNodes": node_idx,
                     "mainNode": 0,
                     "nodeRangeProperties": nodes,
                 },
                 "retryStrategy": {
                     "attempts": max(max(role.max_retries for role in app.roles), 1),
                     "evaluateOnExit": [
                         {"onExitCode": "0", "action": "EXIT"},
@@ -424,34 +513,26 @@
                 },
                 "tags": {
                     TAG_TORCHX_VER: torchx.__version__,
                     TAG_TORCHX_APPNAME: app.name,
                     TAG_TORCHX_USER: cfg.get("user"),
                 },
             },
-            **(
-                {"schedulingPriority": priority if priority is not None else 0}
-                if share_id is not None
-                else {}
-            ),
+            **({"schedulingPriority": priority} if share_id is not None else {}),
         }
 
         req = BatchJob(
             name=name,
             queue=queue,
             share_id=share_id,
             job_def=job_def,
             images_to_push=images_to_push,
         )
         return AppDryRunInfo(req, repr)
 
-    def _validate(self, app: AppDef, scheduler: str) -> None:
-        # Skip validation step
-        pass
-
     def _cancel_existing(self, app_id: str) -> None:
         job_id = self._get_job_id(app_id)
         self._client.terminate_job(
             jobId=job_id,
             reason="killed via torchx CLI",
         )
 
@@ -461,22 +542,30 @@
         opts.add(
             "user",
             type_=str,
             default=getpass.getuser(),
             help="The username to tag the job with. `getpass.getuser()` if not specified.",
         )
         opts.add(
+            "privileged",
+            type_=bool,
+            default=False,
+            help="If true runs the container with elevated permissions."
+            " Equivalent to running with `docker run --privileged`.",
+        )
+        opts.add(
             "share_id",
             type_=str,
             help="The share identifier for the job. "
             "This must be set if and only if the job queue has a scheduling policy.",
         )
         opts.add(
             "priority",
             type_=int,
+            default=0,
             help="The scheduling priority for the job within the context of share_id. "
             "Higher number (between 0 and 9999) means higher priority. "
             "This will only take effect if the job queue has a scheduling policy.",
         )
         return opts
 
     def _get_job_id(self, app_id: str) -> Optional[str]:
@@ -505,39 +594,44 @@
         return jobs[0]
 
     def describe(self, app_id: str) -> Optional[DescribeAppResponse]:
         job = self._get_job(app_id)
         if job is None:
             return None
 
-        # TODO: role statuses
-
-        roles = {}
-        nodes = job["nodeProperties"]["nodeRangeProperties"]
-        for node in nodes:
-            container = node["container"]
+        # each AppDef.role maps to a batch NodeGroup
+        roles = []
+        node_properties = job["nodeProperties"]
+        num_nodes = node_properties["numNodes"]
+        for node_group in node_properties["nodeRangeProperties"]:
+            container = node_group["container"]
             env = {opt["name"]: opt["value"] for opt in container["environment"]}
-            role = env["TORCHX_ROLE_NAME"]
-            replica_id = int(env["TORCHX_REPLICA_IDX"])
 
-            if role not in roles:
-                roles[role] = Role(
-                    name=role,
-                    num_replicas=0,
+            command = container["command"]
+            roles.append(
+                Role(
+                    name=env.get(ENV_TORCHX_ROLE_NAME, DEFAULT_ROLE_NAME),
+                    num_replicas=_parse_num_replicas(
+                        node_group["targetNodes"], num_nodes
+                    ),
                     image=container["image"],
-                    entrypoint=container["command"][0],
-                    args=container["command"][1:],
+                    entrypoint=command[0] if command else MISSING,
+                    args=command[1:],
                     env=env,
+                    resource=resource_from_resource_requirements(
+                        container["resourceRequirements"]
+                    ),
                 )
-            roles[role].num_replicas += 1
+            )
 
         return DescribeAppResponse(
             app_id=app_id,
             state=JOB_STATE[job["status"]],
-            roles=list(roles.values()),
+            roles=roles,
+            # TODO: role statuses
             ui_url=_job_ui_url(job["jobArn"]),
         )
 
     def log_iter(
         self,
         app_id: str,
         role_name: str,
@@ -550,21 +644,29 @@
     ) -> Iterable[str]:
         if streams not in (None, Stream.COMBINED):
             raise ValueError("AWSBatchScheduler only supports COMBINED log stream")
 
         job = self._get_job(app_id)
         if job is None:
             return []
-        nodes = job["nodeProperties"]["nodeRangeProperties"]
+        node_properties = job["nodeProperties"]
+        nodes = node_properties["nodeRangeProperties"]
+
         i = 0
+        # finds the global idx of the node that matches the role's k'th replica
         for i, node in enumerate(nodes):
             container = node["container"]
             env = {opt["name"]: opt["value"] for opt in container["environment"]}
-            node_role = env["TORCHX_ROLE_NAME"]
-            replica_id = int(env["TORCHX_REPLICA_IDX"])
+            node_role = env.get(ENV_TORCHX_ROLE_NAME, DEFAULT_ROLE_NAME)
+            start_idx, _ = _parse_start_and_end_idx(
+                node["targetNodes"],
+                node_properties["numNodes"],
+            )
+            replica_id = start_idx + k
+
             if role_name == node_role and k == replica_id:
                 break
 
         job = self._get_job(app_id, rank=i)
         if not job:
             return []
 
@@ -575,15 +677,21 @@
             if len(attempts) == 0:
                 return []
 
             attempt = attempts[-1]
             container = attempt["container"]
             stream_name = container["logStreamName"]
 
-        iterator = self._stream_events(stream_name, since=since, until=until)
+        iterator = self._stream_events(
+            app_id,
+            stream_name,
+            since=since,
+            until=until,
+            should_tail=should_tail,
+        )
         if regex:
             return filter_regex(regex, iterator)
         else:
             return iterator
 
     def list(self) -> List[ListAppResponse]:
         # TODO: get queue name input instead of iterating over all queues?
@@ -605,15 +713,14 @@
         jobs = []
         for resp in self._client.get_paginator("list_jobs").paginate(
             jobQueue=queue_name,
             filters=[EVERY_STATUS],
             # describe-jobs API can take up to 100 jobIds
             PaginationConfig={"MaxItems": 100},
         ):
-
             # torchx.pytorch.org/version tag is used to filter torchx jobs
             # list_jobs() API only returns a job summary which does not include the job's tag
             # so we need to call the describe_jobs API.
             # Ideally batch lets us pass tags as a filter to list_jobs API
             # but this is currently not supported
             job_ids = [js["jobId"] for js in resp["jobSummaryList"]]
             for jobdesc in self._get_torchx_submitted_jobs(job_ids):
@@ -634,44 +741,54 @@
             jobdesc
             for jobdesc in self._client.describe_jobs(jobs=job_ids)["jobs"]
             if TAG_TORCHX_VER in jobdesc["tags"]
         ]
 
     def _stream_events(
         self,
+        app_id: str,
         stream_name: str,
         since: Optional[datetime] = None,
         until: Optional[datetime] = None,
+        should_tail: bool = False,
     ) -> Iterable[str]:
-
         next_token = None
+        last_event_timestamp: int = 0  # in millis since epoch
 
         while True:
             args = {}
             if next_token is not None:
                 args["nextToken"] = next_token
             if until is not None:
-                args["endTime"] = until.timestamp()
+                args["endTime"] = to_millis_since_epoch(until)
             if since is not None:
-                args["startTime"] = since.timestamp()
+                args["startTime"] = to_millis_since_epoch(since)
             try:
                 response = self._log_client.get_log_events(
                     logGroupName="/aws/batch/job",
                     logStreamName=stream_name,
                     limit=10000,
                     startFromHead=True,
                     **args,
                 )
             except self._log_client.exceptions.ResourceNotFoundException:
                 return []  # noqa: B901
             if response["nextForwardToken"] == next_token:
+                if (
+                    not until or last_event_timestamp < to_millis_since_epoch(until)
+                ) and should_tail:
+                    if not is_terminal(none_throws(self.describe(app_id)).state):
+                        since = to_datetime(last_event_timestamp)
+                        continue
                 break
+
             next_token = response["nextForwardToken"]
 
             for event in response["events"]:
+                last_event_timestamp = event["timestamp"]
                 yield event["message"] + "\n"
 
 
 def create_scheduler(session_name: str, **kwargs: object) -> AWSBatchScheduler:
     return AWSBatchScheduler(
         session_name=session_name,
     )
```

## torchx/schedulers/docker_scheduler.py

```diff
@@ -39,14 +39,15 @@
     VolumeMount,
 )
 from torchx.workspace.docker_workspace import DockerWorkspaceMixin
 from typing_extensions import TypedDict
 
 
 if TYPE_CHECKING:
+    from docker import DockerClient
     from docker.models.containers import Container
 
 log: logging.Logger = logging.getLogger(__name__)
 
 CONTAINER_STATE: Dict[str, AppState] = {
     "created": AppState.SUBMITTED,
     "restarting": AppState.PENDING,
@@ -90,14 +91,38 @@
 
         docker.from_env()
         return True
     except (ImportError, docker.errors.DockerException):
         return False
 
 
+def ensure_network(client: Optional["DockerClient"] = None) -> None:
+    """
+    This creates the torchx docker network. Multi-process safe.
+    """
+    import filelock
+    from docker.errors import APIError
+
+    if client is None:
+        import docker
+
+        client = docker.from_env()
+
+    lock_path = os.path.join(tempfile.gettempdir(), "torchx_docker_network_lock")
+
+    # Docker networks.create check_duplicate has a race condition so we need
+    # to do client side locking to ensure only one network is created.
+    with filelock.FileLock(lock_path, timeout=10):
+        try:
+            client.networks.create(name=NETWORK, driver="bridge", check_duplicate=True)
+        except APIError as e:
+            if "already exists" not in str(e):
+                raise
+
+
 class DockerOpts(TypedDict, total=False):
     copy_env: Optional[List[str]]
 
 
 class DockerScheduler(DockerWorkspaceMixin, Scheduler[DockerOpts]):
     """
     DockerScheduler is a TorchX scheduling interface to Docker.
@@ -141,32 +166,14 @@
             mounts: true
             elasticity: false
     """
 
     def __init__(self, session_name: str) -> None:
         super().__init__("docker", session_name)
 
-    def _ensure_network(self) -> None:
-        import filelock
-        from docker.errors import APIError
-
-        client = self._docker_client
-        lock_path = os.path.join(tempfile.gettempdir(), "torchx_docker_network_lock")
-
-        # Docker networks.create check_duplicate has a race condition so we need
-        # to do client side locking to ensure only one network is created.
-        with filelock.FileLock(lock_path, timeout=10):
-            try:
-                client.networks.create(
-                    name=NETWORK, driver="bridge", check_duplicate=True
-                )
-            except APIError as e:
-                if "already exists" not in str(e):
-                    raise
-
     def schedule(self, dryrun_info: AppDryRunInfo[DockerJob]) -> str:
         client = self._docker_client
 
         req = dryrun_info.request
 
         images = set()
         for container in req.containers:
@@ -176,15 +183,15 @@
                 continue
             log.info(f"Pulling container image: {image} (this may take a while)")
             try:
                 client.images.pull(image)
             except Exception as e:
                 log.warning(f"failed to pull image {image}, falling back to local: {e}")
 
-        self._ensure_network()
+        ensure_network(self._docker_client)
 
         for container in req.containers:
             client.containers.run(
                 container.image,
                 container.command,
                 detach=True,
                 **container.kwargs,
```

## torchx/schedulers/kubernetes_mcad_scheduler.py

```diff
@@ -137,21 +137,24 @@
     # least one container has terminated in a failure (exited with a
     # non-zero exit code or was stopped by the system).
     "failed": ReplicaState.FAILED,
     # Unknown means the status of task/pod is unknown to the scheduler.
     "Unknown": ReplicaState.UNKNOWN,
 }
 
-# TO DO update to standards
 LABEL_VERSION = "torchx.pytorch.org/version"
 LABEL_APP_NAME = "torchx.pytorch.org/app-name"
 LABEL_ROLE_INDEX = "torchx.pytorch.org/role-index"
 LABEL_ROLE_NAME = "torchx.pytorch.org/role-name"
 LABEL_REPLICA_ID = "torchx.pytorch.org/replica-id"
 
+LABEL_KUBE_APP_NAME = "app.kubernetes.io/name"
+LABEL_ORGANIZATION = "app.kubernetes.io/managed-by"
+LABEL_UNIQUE_NAME = "app.kubernetes.io/instance"
+
 ANNOTATION_ISTIO_SIDECAR = "sidecar.istio.io/inject"
 
 LABEL_INSTANCE_TYPE = "node.kubernetes.io/instance-type"
 
 
 def sanitize_for_serialization(obj: object) -> object:
     from kubernetes import client
@@ -164,14 +167,16 @@
     name: str,
     unique_app_id: str,
     namespace: str,
     role: Role,
     service_account: Optional[str],
     image_secret: Optional[str],
     coscheduler_name: Optional[str],
+    priority_class_name: Optional[str],
+    network: Optional[str],
 ) -> "V1Pod":
     from kubernetes.client.models import (  # noqa: F811 redefinition of unused
         V1Container,
         V1ContainerPort,
         V1EmptyDirVolumeSource,
         V1EnvVar,
         V1HostPathVolumeSource,
@@ -322,67 +327,77 @@
         ],
         volume_mounts=volume_mounts,
         security_context=security_context,
     )
 
     # Get correct formatting for image secret
     imagesecret = V1LocalObjectReference(name=image_secret)
+    metadata = V1ObjectMeta(
+        name=name,
+        annotations={
+            # Disable the istio sidecar as it prevents the containers from
+            # exiting once finished.
+            ANNOTATION_ISTIO_SIDECAR: "false",
+        },
+        labels={},
+        namespace=namespace,
+    )
+    if network is not None:
+        metadata.annotations.update({"k8s.v1.cni.cncf.io/networks": network})
+
     return V1Pod(
         api_version="v1",
         kind="Pod",
         spec=V1PodSpec(
             containers=[container],
             hostname=name,
             subdomain=unique_app_id,
             image_pull_secrets=[imagesecret],
             restart_policy="Never",
             service_account_name=service_account,
             volumes=volumes,
             node_selector=node_selector,
             scheduler_name=coscheduler_name,
+            priority_class_name=priority_class_name,
         ),
-        metadata=V1ObjectMeta(
-            name=name,
-            annotations={
-                # Disable the istio sidecar as it prevents the containers from
-                # exiting once finished.
-                ANNOTATION_ISTIO_SIDECAR: "false",
-            },
-            labels={},
-            namespace=namespace,
-        ),
+        metadata=metadata,
     )
 
 
-def create_pod_group(role: Role, namespace: str, app_id: str) -> "Dict[str, Any]":
+def create_pod_group(
+    app: AppDef, role: Role, namespace: str, app_id: str
+) -> "Dict[str, Any]":
     pod_group_name = app_id + "-" + cleanup_str(role.name) + "-pg"
 
+    labels = object_labels(app, app_id)
+    labels.update({"appwrapper.mcad.ibm.com": app_id})
+
     pod_group: Dict[str, Any] = {
         "apiVersion": "scheduling.sigs.k8s.io/v1alpha1",
         "kind": "PodGroup",
         "metadata": {
             "name": pod_group_name,
             "namespace": namespace,
-            "labels": {
-                "appwrapper.mcad.ibm.com": app_id,
-            },
+            "labels": labels,
         },
         "spec": {
             "minMember": role.num_replicas,
         },
     }
 
     genericitem_pod_group: Dict[str, Any] = {
         "replicas": 1,
         "generictemplate": pod_group,
     }
     return genericitem_pod_group
 
 
-def mcad_svc(svc_name: str, namespace: str, service_port: str) -> "V1Service":
+def mcad_svc(
+    app: AppDef, svc_name: str, namespace: str, service_port: str
+) -> "V1Service":
     from kubernetes.client.models import (  # noqa: F401, F811
         V1Container,
         V1ContainerPort,
         V1EmptyDirVolumeSource,
         V1EnvVar,
         V1HostPathVolumeSource,
         V1ObjectMeta,
@@ -395,20 +410,23 @@
         V1ServicePort,
         V1ServiceSpec,
         V1ServiceStatus,
         V1Volume,
         V1VolumeMount,
     )
 
+    labels = object_labels(app, svc_name)
+
     return V1Service(
         api_version="v1",
         kind="Service",
         metadata=V1ObjectMeta(
             name=svc_name,
             namespace=namespace,
+            labels=labels,
         ),
         spec=V1ServiceSpec(
             cluster_ip="None",
             publish_not_ready_addresses=True,
             ports=[
                 V1ServicePort(
                     protocol="TCP",
@@ -432,15 +450,15 @@
     characters that do not satisfy ``[a-z0-9]`` pattern.
     This method is mostly used to make sure kubernetes scheduler gets
     the job name that does not violate its validation.
     """
     if data.startswith("-"):
         data = data[1:]
     pattern = r"[a-z0-9\-]"
-    return "".join(re.findall(pattern, data.lower()))
+    return "".join(re.findall(pattern, data.lower())).lstrip("0123456789")
 
 
 def get_port_for_service(app: AppDef) -> str:
     # Initialize port to default
     port = "29500"
 
     for role_idx, role in enumerate(app.roles):
@@ -459,14 +477,16 @@
 
 def app_to_resource(
     app: AppDef,
     namespace: str,
     service_account: Optional[str],
     image_secret: Optional[str],
     coscheduler_name: Optional[str],
+    priority_class_name: Optional[str],
+    network: Optional[str],
     priority: Optional[int] = None,
 ) -> Dict[str, Any]:
     """
     app_to_resource creates a AppWrapper/MCAD Kubernetes resource definition from
     the provided AppDef. The resource definition can be used to launch the
     app on Kubernetes.
 
@@ -474,15 +494,17 @@
 
     genericitems = []
 
     unique_app_id = cleanup_str(make_unique(app.name))
 
     if coscheduler_name is not None:
         for role_idx, role in enumerate(app.roles):
-            genericitem_pod_group = create_pod_group(role, namespace, unique_app_id)
+            genericitem_pod_group = create_pod_group(
+                app, role, namespace, unique_app_id
+            )
             genericitems.append(genericitem_pod_group)
 
     for role_idx, role in enumerate(app.roles):
         for replica_id in range(role.num_replicas):
             values = macros.Values(
                 img_root="",
                 app_id=unique_app_id,
@@ -503,14 +525,16 @@
                 name,
                 unique_app_id,
                 namespace,
                 replica_role,
                 service_account,
                 image_secret,
                 coscheduler_name,
+                priority_class_name,
+                network,
             )
             pod.metadata.labels.update(
                 pod_labels(
                     app=app,
                     role_idx=role_idx,
                     role=role,
                     replica_id=replica_id,
@@ -538,15 +562,17 @@
     Create Service:
     The selector will have the key 'appwrapper.mcad.ibm.com', and the value will be 
     the appwrapper name
     """
 
     service_port = get_port_for_service(app)
 
-    svc_obj = mcad_svc(unique_app_id, namespace, service_port)
+    svc_obj = mcad_svc(
+        app=app, svc_name=unique_app_id, namespace=namespace, service_port=service_port
+    )
 
     genericitem_svc: Dict[str, Any] = {
         "replicas": 1,
         "generictemplate": svc_obj,
     }
     genericitems.append(genericitem_svc)
 
@@ -558,15 +584,15 @@
 
     if priority is not None:
         job_spec["priority"] = priority
 
     resource: Dict[str, object] = {
         "apiVersion": "mcad.ibm.com/v1beta1",
         "kind": "AppWrapper",
-        "metadata": {"name": unique_app_id},
+        "metadata": {"name": unique_app_id, "namespace": namespace},
         "spec": job_spec,
     }
     return resource
 
 
 # Helper function for MCAD generic items information -> TorchX Role
 def get_role_information(generic_items: Iterable[Dict[str, Any]]) -> Dict[str, Any]:
@@ -714,16 +740,18 @@
 
 
 class KubernetesMCADOpts(TypedDict, total=False):
     namespace: Optional[str]
     image_repo: Optional[str]
     service_account: Optional[str]
     priority: Optional[int]
+    priority_class_name: Optional[str]
     image_secret: Optional[str]
     coscheduler_name: Optional[str]
+    network: Optional[str]
 
 
 class KubernetesMCADScheduler(DockerWorkspaceMixin, Scheduler[KubernetesMCADOpts]):
     """
     KubernetesMCADScheduler is a TorchX scheduling interface to Kubernetes.
 
     Important: AppWrapper/MCAD is required to be installed on the Kubernetes cluster.
@@ -747,14 +775,24 @@
     To enable this, the user must provide the name of the coscheduler.
     With this feature, a PodGroup is defined for each TorchX role and the coscheduler
     handles secondary scheduling on the Kubernetes cluster. For additional resources, see:
     1. PodGroups and Coscheduling: https://github.com/kubernetes-sigs/scheduler-plugins/tree/release-1.24/pkg/coscheduling
     2. Installing Secondary schedulers: https://github.com/kubernetes-sigs/scheduler-plugins/blob/release-1.24/doc/install.md
     3. PodGroup CRD: https://github.com/kubernetes-sigs/scheduler-plugins/blob/release-1.24/config/crd/bases/scheduling.sigs.k8s.io_podgroups.yaml
 
+    The MCAD scheduler supports priorities at the AppWrapper level and optionally at the pod level on clusters with PriorityClass definitions.
+    At the AppWrapper level, higher integer values means higher priorities. Kubernetes clusters may have additional priorityClass
+    definitions that can be applied at the pod level. While these different levels of priorities can be set independently,
+    it is recommended to check with your Kubernetes cluster admin to see if additional guidance is in place. For more on Kubernetes
+    PriorityClass, see: https://kubernetes.io/docs/concepts/scheduling-eviction/pod-priority-preemption/ .
+
+    In order to use the network option, the Kubernetes cluster must have multus installed.
+    For multus installation instructions and how to set up a network custom network attachment definition, see:
+    https://github.com/k8snetworkplumbingwg/multus-cni/blob/master/docs/how-to-use.md
+
     **Config Options**
 
     .. runopts::
         class: torchx.schedulers.kubernetes_mcad_scheduler.KubernetesMCADScheduler
 
     **Mounts**
 
@@ -914,20 +952,30 @@
         assert isinstance(namespace, str), "namespace must be a str"
 
         coscheduler_name = cfg.get("coscheduler_name")
         assert coscheduler_name is None or isinstance(
             coscheduler_name, str
         ), "coscheduler_name must be a string"
 
+        priority_class_name = cfg.get("priority_class_name")
+        assert priority_class_name is None or isinstance(
+            priority_class_name, str
+        ), "priority_class_name must be a string"
+
+        network = cfg.get("network")
+        assert network is None or isinstance(network, str), "network must be a string"
+
         resource = app_to_resource(
             app=app,
             namespace=namespace,
             service_account=service_account,
             image_secret=image_secret,
             coscheduler_name=coscheduler_name,
+            priority_class_name=priority_class_name,
+            network=network,
             priority=priority,
         )
 
         req = KubernetesMCADJob(
             resource=resource,
             images_to_push=images_to_push,
         )
@@ -972,23 +1020,33 @@
         )
         opts.add(
             "priority",
             type_=int,
             help="The priority level to set on the job specs. Higher integer value means higher priority",
         )
         opts.add(
+            "priority_class_name",
+            type_=str,
+            help="Pod specific priority level. Check with your Kubernetes cluster admin if Priority classes are defined on your system",
+        )
+        opts.add(
             "image_secret",
             type_=str,
             help="The name of the Kubernetes/OpenShift secret set up for private images",
         )
         opts.add(
             "coscheduler_name",
             type_=str,
             help="Option to run TorchX-MCAD with a co-scheduler. User must provide the co-scheduler name.",
         )
+        opts.add(
+            "network",
+            type_=str,
+            help="Name of additional pod-to-pod network beyond default Kubernetes network",
+        )
         return opts
 
     def describe(self, app_id: str) -> Optional[DescribeAppResponse]:
         namespace, name = app_id.split(":")
         from kubernetes.client.rest import ApiException
 
         roles = {}
@@ -1132,27 +1190,40 @@
 
 def create_scheduler(session_name: str, **kwargs: Any) -> KubernetesMCADScheduler:
     return KubernetesMCADScheduler(
         session_name=session_name,
     )
 
 
-# TODO update to Kubernetes standard labels (https://kubernetes.io/docs/concepts/overview/working-with-objects/common-labels/)
+def object_labels(
+    app: AppDef,
+    app_id: str,
+) -> Dict[str, str]:
+    return {
+        LABEL_KUBE_APP_NAME: app.name,
+        LABEL_ORGANIZATION: "torchx.pytorch.org",
+        LABEL_UNIQUE_NAME: app_id,
+    }
+
+
 def pod_labels(
     app: AppDef,
     role_idx: int,
     role: Role,
     replica_id: int,
     coscheduler_name: Optional[str],
     app_id: str,
 ) -> Dict[str, str]:
-    labels = {
+    labels = object_labels(app, app_id)
+    pod_labels = {
         LABEL_VERSION: torchx.__version__,
         LABEL_APP_NAME: app.name,
         LABEL_ROLE_INDEX: str(role_idx),
         LABEL_ROLE_NAME: role.name,
         LABEL_REPLICA_ID: str(replica_id),
     }
     if coscheduler_name is not None:
         pod_group = app_id + "-" + cleanup_str(role.name) + "-pg"
-        labels.update({"pod-group.scheduling.sigs.k8s.io": pod_group})
+        pod_labels.update({"pod-group.scheduling.sigs.k8s.io": pod_group})
+
+    labels.update(pod_labels)
     return labels
```

## torchx/schedulers/kubernetes_scheduler.py

```diff
@@ -155,14 +155,17 @@
 }
 
 LABEL_VERSION = "torchx.pytorch.org/version"
 LABEL_APP_NAME = "torchx.pytorch.org/app-name"
 LABEL_ROLE_INDEX = "torchx.pytorch.org/role-index"
 LABEL_ROLE_NAME = "torchx.pytorch.org/role-name"
 LABEL_REPLICA_ID = "torchx.pytorch.org/replica-id"
+LABEL_KUBE_APP_NAME = "app.kubernetes.io/name"
+LABEL_ORGANIZATION = "app.kubernetes.io/managed-by"
+LABEL_UNIQUE_NAME = "app.kubernetes.io/instance"
 
 ANNOTATION_ISTIO_SIDECAR = "sidecar.istio.io/inject"
 
 LABEL_INSTANCE_TYPE = "node.kubernetes.io/instance-type"
 
 
 def sanitize_for_serialization(obj: object) -> object:
@@ -368,15 +371,23 @@
                 values.rank0_env = "TORCHX_RANK0_HOST"
             name = normalize_str(f"{role.name}-{replica_id}")
             replica_role = values.apply(role)
             if role_idx == 0 and replica_id == 0:
                 replica_role.env["TORCHX_RANK0_HOST"] = "localhost"
 
             pod = role_to_pod(name, replica_role, service_account)
-            pod.metadata.labels.update(pod_labels(app, role_idx, role, replica_id))
+            pod.metadata.labels.update(
+                pod_labels(
+                    app=app,
+                    role_idx=role_idx,
+                    role=role,
+                    replica_id=replica_id,
+                    app_id=unique_app_id,
+                )
+            )
             task: Dict[str, Any] = {
                 "replicas": 1,
                 "name": name,
                 "template": pod,
             }
             if role.max_retries > 0:
                 task["maxRetry"] = role.max_retries
@@ -769,16 +780,19 @@
 def create_scheduler(session_name: str, **kwargs: Any) -> KubernetesScheduler:
     return KubernetesScheduler(
         session_name=session_name,
     )
 
 
 def pod_labels(
-    app: AppDef, role_idx: int, role: Role, replica_id: int
+    app: AppDef, role_idx: int, role: Role, replica_id: int, app_id: str
 ) -> Dict[str, str]:
     return {
         LABEL_VERSION: torchx.__version__,
         LABEL_APP_NAME: app.name,
         LABEL_ROLE_INDEX: str(role_idx),
         LABEL_ROLE_NAME: role.name,
         LABEL_REPLICA_ID: str(replica_id),
+        LABEL_KUBE_APP_NAME: app.name,
+        LABEL_ORGANIZATION: "torchx.pytorch.org",
+        LABEL_UNIQUE_NAME: app_id,
     }
```

## torchx/schedulers/local_scheduler.py

```diff
@@ -434,15 +434,15 @@
         with open(os.path.join(self.log_dir, "SUCCESS"), "w") as fp:
             fp.write(info_str)
 
         log.debug(f"Successfully closed app_id: {self.id}.\n{info_str}")
 
     def __repr__(self) -> str:
         role_to_pid = {}
-        for (role_name, replicas) in self.role_replicas.items():
+        for role_name, replicas in self.role_replicas.items():
             pids = role_to_pid.setdefault(role_name, [])
             for r in replicas:
                 pids.append(r.proc.pid)
 
         return f"{{app_id:{self.id}, state:{self.state}, pid_map:{role_to_pid}}}"
 
 
@@ -608,15 +608,15 @@
 
         Returns:
             ``True`` if an entry was evicted, ``False`` if no entries could be evicted
             (e.g. all apps are running)
         """
         lru_time = sys.maxsize
         lru_app_id = None
-        for (app_id, app) in self._apps.items():
+        for app_id, app in self._apps.items():
             if is_terminal(app.state):
                 if app.last_updated <= lru_time:
                     lru_app_id = app_id
 
         if lru_app_id:
             # evict LRU finished app from the apps cache
             del self._apps[lru_app_id]
@@ -984,15 +984,15 @@
         # can assume app_id exists
         local_app = self._apps[app_id]
         local_app.close()
         local_app.state = AppState.CANCELLED
 
     def close(self) -> None:
         # terminate all apps
-        for (app_id, app) in self._apps.items():
+        for app_id, app in self._apps.items():
             log.debug(f"Terminating app: {app_id}")
             app.kill()
         # delete logdir if torchx created a log dir
         if self._base_log_dir and self._created_tmp_log_dir:
             shutil.rmtree(self._base_log_dir, ignore_errors=True)
 
     def __del__(self) -> None:
@@ -1033,15 +1033,20 @@
 
     def __iter__(self) -> "LogIterator":
         # wait for the log file to appear or app to finish (whichever happens first)
         while True:
             self._check_finished()  # check to see if app has finished running
 
             if os.path.isfile(self._log_file):
-                self._log_fp = open(self._log_file, "rt", newline="\n")  # noqa: P201
+                self._log_fp = open(
+                    self._log_file,
+                    mode="rt",
+                    newline="\n",
+                    errors="replace",  # replace bad utf-8 with \uFFFD
+                )  # noqa: P201
                 break
 
             if self._app_finished:
                 # app finished without ever writing a log file
                 raise RuntimeError(
                     f"app: {self._app_id} finished without writing: {self._log_file}"
                 )
```

## torchx/schedulers/lsf_scheduler.py

```diff
@@ -15,15 +15,15 @@
 Prerequisites
 ==============
 
 You'll need either an existing LSF cluster to run your jobs or for individuals
 you can install LSF Community Edition.
 
 See the LSF documentation for more details:
-https://www.ibm.com/support/pages/where-do-i-download-lsf-community-edition
+https://www.ibm.com/docs/en/cloud-private/3.2.x?topic=paks-spectrum-lsf-community-edition
 """
 import os.path
 import re
 import subprocess
 import tempfile
 from dataclasses import dataclass
 from datetime import datetime
```

## torchx/schedulers/ray_scheduler.py

```diff
@@ -54,15 +54,14 @@
     cluster_name: Optional[str]
     dashboard_address: Optional[str]
     working_dir: Optional[str]
     requirements: Optional[str]
 
 
 if _has_ray:
-
     _logger: logging.Logger = logging.getLogger(__name__)
 
     _ray_status_to_torchx_appstate: Dict[JobStatus, AppState] = {
         JobStatus.PENDING: AppState.PENDING,
         JobStatus.RUNNING: AppState.RUNNING,
         JobStatus.SUCCEEDED: AppState.SUCCEEDED,
         JobStatus.FAILED: AppState.FAILED,
```

## torchx/specs/__init__.py

```diff
@@ -10,14 +10,17 @@
 used by components to define the apps which can then be launched via a TorchX
 scheduler or pipeline adapter.
 """
 import difflib
 from typing import Callable, Dict, Optional
 
 from torchx.specs.named_resources_aws import NAMED_RESOURCES as AWS_NAMED_RESOURCES
+from torchx.specs.named_resources_generic import (
+    NAMED_RESOURCES as GENERIC_NAMED_RESOURCES,
+)
 from torchx.util.entrypoints import load_group
 
 from .api import (  # noqa: F401 F403
     ALL,
     AppDef,
     AppDryRunInfo,
     AppHandle,
@@ -51,20 +54,24 @@
 
 GiB: int = 1024
 
 
 def _load_named_resources() -> Dict[str, Callable[[], Resource]]:
     resource_methods = load_group("torchx.named_resources", default={})
     materialized_resources: Dict[str, Callable[[], Resource]] = {}
-    default = AWS_NAMED_RESOURCES
-    for name, resource in default.items():
+
+    for name, resource in {
+        **GENERIC_NAMED_RESOURCES,
+        **AWS_NAMED_RESOURCES,
+        **resource_methods,
+    }.items():
         materialized_resources[name] = resource
-    for resource_name, resource_method in resource_methods.items():
-        materialized_resources[resource_name] = resource_method
+
     materialized_resources["NULL"] = lambda: NULL_RESOURCE
+    materialized_resources["MISSING"] = lambda: NULL_RESOURCE
     return materialized_resources
 
 
 _named_resource_factories: Dict[str, Callable[[], Resource]] = _load_named_resources()
 
 
 class _NamedResourcesLibrary:
```

## torchx/specs/finder.py

```diff
@@ -219,15 +219,17 @@
         else:
             return module
 
     def _get_components_from_module(self, module: ModuleType) -> List[_Component]:
         functions = getmembers(module, isfunction)
         component_defs = []
 
-        module_path = os.path.abspath(module.__file__)
+        module_path = module.__file__
+        assert module_path, f"module must have __file__: {module_path}"
+        module_path = os.path.abspath(module_path)
         rel_module_name = module_relname(module, relative_to=self.base_module)
         for function_name, function in functions:
             linter_errors = validate(module_path, function_name)
             component_desc, _ = get_fn_docstring(function)
 
             # remove empty string to deal with group=""
             component_name = ".".join(
```

## torchx/tracker/__init__.py

```diff
@@ -87,16 +87,16 @@
 
 .. code-block:: ini
 
     [torchx.tracker]
     entry_point_name=my_module:create_tracker_fn
 
 
-Aquiring :py:class:`~torchx.tracker.api.AppRun` instance
---------------------------------------------------------
+Acquiring :py:class:`~torchx.tracker.api.AppRun` instance
+-------------------------------------------------------------
 
 Use :py:meth:`~torchx.tracker.app_run_from_env`:
 
 
     >>> import os; os.environ["TORCHX_JOB_ID"] = "scheduler://session/job_id" # Simulate running job first
     >>> from torchx.tracker import app_run_from_env
     >>> app_run = app_run_from_env()
```

## torchx/tracker/api.py

```diff
@@ -13,16 +13,17 @@
 from functools import lru_cache
 from typing import Iterable, Mapping, Optional
 
 from torchx.util.entrypoints import load_group
 
 logger: logging.Logger = logging.getLogger(__name__)
 
-TRACKER_ENV_VAR_NAME = "TORCHX_TRACKERS"
-TRACKER_PARENT_RUN_ENV_VAR_NAME = "TORCHX_PARENT_RUN_ID"
+ENV_TORCHX_TRACKERS = "TORCHX_TRACKERS"
+ENV_TORCHX_PARENT_RUN_ID = "TORCHX_PARENT_RUN_ID"
+ENV_TORCHX_JOB_ID = "TORCHX_JOB_ID"
 
 
 @dataclass
 class TrackerSource:
     """
     Dataclass to represent sources at backend tracker level
 
@@ -153,20 +154,20 @@
 
 def tracker_config_env_var_name(entrypoint_key: str) -> str:
     """Utility method to derive tracker config env variable name given tracker name"""
     return f"TORCHX_TRACKER_{entrypoint_key.upper()}_CONFIG"
 
 
 def _extract_tracker_name_and_config_from_environ() -> Mapping[str, Optional[str]]:
-    if TRACKER_ENV_VAR_NAME not in os.environ:
+    if ENV_TORCHX_TRACKERS not in os.environ:
         logger.info("No trackers were configured, skipping setup.")
         return {}
 
-    tracker_backend_entrypoints = os.environ[TRACKER_ENV_VAR_NAME]
-    logger.info(f"Trackers specified {tracker_backend_entrypoints}")
+    tracker_backend_entrypoints = os.environ[ENV_TORCHX_TRACKERS]
+    logger.info(f"Trackers: {ENV_TORCHX_TRACKERS}={tracker_backend_entrypoints}")
 
     entries = {}
     for entrypoint_key in tracker_backend_entrypoints.split(","):
         config = None
         config_env_name = tracker_config_env_var_name(entrypoint_key)
         if config_env_name in os.environ:
             config = os.environ[config_env_name]
@@ -178,32 +179,31 @@
 def build_trackers(
     entrypoint_and_config: Mapping[str, Optional[str]]
 ) -> Iterable[TrackerBase]:
     trackers = []
 
     entrypoint_factories = load_group("torchx.tracker")
     if not entrypoint_factories:
-        logger.warn(
+        logger.warning(
             "No 'torchx.tracker' entry_points are defined. Tracking will not capture any data."
         )
         return trackers
 
     for entrypoint_key, config in entrypoint_and_config.items():
-        logger.info(f"Configuring tracker {entrypoint_key}")
         if entrypoint_key not in entrypoint_factories:
-            logger.warn(
-                f"Coult not find '{entrypoint_key}' tracker entrypoint, skipping."
+            logger.warning(
+                f"Could not find `{entrypoint_key}` tracker entrypoint. Skipping..."
             )
             continue
         factory = entrypoint_factories[entrypoint_key]
         if config:
-            logger.info(f"Trackers config specified for {entrypoint_key} as {config}")
+            logger.info(f"Tracker config found for `{entrypoint_key}` as `{config}`")
             tracker = factory(config)
         else:
-            logger.info(f"No trackers config specified for {entrypoint_key}")
+            logger.info(f"No tracker config specified for `{entrypoint_key}`")
             tracker = factory(None)
         trackers.append(tracker)
     return trackers
 
 
 def trackers_from_environ() -> Iterable[TrackerBase]:
     """
@@ -261,31 +261,31 @@
                 No trackers are hooked up to the ``AppRun`` hence
                 calling ``add_*()`` (write) methods on the returned apprun will be a no-op.
 
         Usage:
 
         .. doctest::
 
-            >>> from torchx.tracker.api import AppRun
+            >>> from torchx.mock_tracker.api import AppRun
             >>> apprun = AppRun.run_from_env()
             >>> apprun.add_metadata(md_1 = "foo", md_2 = "bar")
 
 
         """
 
-        torchx_job_id = os.getenv("TORCHX_JOB_ID", default="<UNDEFINED>")
+        torchx_job_id = os.getenv(ENV_TORCHX_JOB_ID, default="<UNDEFINED>")
 
         trackers = trackers_from_environ()
-        if TRACKER_PARENT_RUN_ENV_VAR_NAME in os.environ:
-            parent_run_id = os.environ[TRACKER_PARENT_RUN_ENV_VAR_NAME]
+        if ENV_TORCHX_PARENT_RUN_ID in os.environ:
+            parent_run_id = os.environ[ENV_TORCHX_PARENT_RUN_ID]
             logger.info(f"Tracker parent run ID: '{parent_run_id}'")
             for tracker in trackers:
                 tracker.add_source(torchx_job_id, parent_run_id, artifact_name=None)
 
-        return AppRun(id=torchx_job_id, backends=trackers_from_environ())
+        return AppRun(id=torchx_job_id, backends=trackers)
 
     def add_metadata(self, **kwargs: object) -> None:
         """Stores metadata for the current run"""
         for backend in self.backends:
             backend.add_metadata(self.id, **kwargs)
 
     def add_artifact(
```

## torchx/util/strings.py

```diff
@@ -6,15 +6,15 @@
 
 import re
 
 
 def normalize_str(data: str) -> str:
     """
     Invokes ``lower`` on thes string and removes all
-    characters that do not satisfy ``[a-z0-9]`` pattern.
+    characters that do not satisfy ``[a-z0-9\\-]`` pattern.
     This method is mostly used to make sure kubernetes and gcp_batch scheduler gets
     the job name that does not violate its restrictions.
     """
     if data.startswith("-"):
         data = data[1:]
     pattern = r"[a-z0-9\-]"
     return "".join(re.findall(pattern, data.lower()))
```

## Comparing `codeflare_torchx-0.5.0.dev5.dist-info/LICENSE` & `codeflare_torchx-0.6.0.dev0.dist-info/LICENSE`

 * *Files 22% similar despite different names*

```diff
@@ -1,12 +1,21 @@
 BSD 3-Clause License
 
+All contributions by Meta:
 Copyright (c) Meta Platforms, Inc. and affiliates.
 All rights reserved.
 
+All contributions by Kiuk Chung:
+Copyright (c) Kiuk Chung
+All rights reserved.
+
+All contributions by Tristan Rice:
+Copyright (c) Tristan Rice
+All rights reserved.
+
 Redistribution and use in source and binary forms, with or without
 modification, are permitted provided that the following conditions are met:
 
 * Redistributions of source code must retain the above copyright notice, this
   list of conditions and the following disclaimer.
 
 * Redistributions in binary form must reproduce the above copyright notice,
```

## Comparing `codeflare_torchx-0.5.0.dev5.dist-info/METADATA` & `codeflare_torchx-0.6.0.dev0.dist-info/METADATA`

 * *Files 3% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: codeflare-torchx
-Version: 0.5.0.dev5
+Version: 0.6.0.dev0
 Summary: TorchX SDK and Components
 Home-page: https://github.com/pytorch/torchx
 Author: TorchX Devs
 Author-email: torchx@fb.com
 License: BSD-3
 Keywords: pytorch,machine learning
 Classifier: Development Status :: 4 - Beta
@@ -25,28 +25,30 @@
 Requires-Dist: filelock
 Requires-Dist: fsspec
 Requires-Dist: urllib3 (<1.27,>=1.21.1)
 Requires-Dist: tabulate
 Provides-Extra: dev
 Requires-Dist: aiobotocore (==2.4.2) ; extra == 'dev'
 Requires-Dist: ax-platform[mysql] (==0.2.3) ; extra == 'dev'
-Requires-Dist: black (==22.3.0) ; extra == 'dev'
+Requires-Dist: black (==23.3.0) ; extra == 'dev'
 Requires-Dist: boto3 (==1.24.59) ; extra == 'dev'
 Requires-Dist: captum (>=0.4.0) ; extra == 'dev'
 Requires-Dist: flake8 (==3.9.0) ; extra == 'dev'
 Requires-Dist: fsspec[s3] (==2023.1.0) ; extra == 'dev'
 Requires-Dist: google-api-core ; extra == 'dev'
 Requires-Dist: google-cloud-batch (>=0.5.0) ; extra == 'dev'
 Requires-Dist: google-cloud-logging (>=3.0.0) ; extra == 'dev'
 Requires-Dist: google-cloud-runtimeconfig (>=0.33.2) ; extra == 'dev'
 Requires-Dist: hydra-core ; extra == 'dev'
 Requires-Dist: ipython ; extra == 'dev'
 Requires-Dist: kfp (==1.8.9) ; extra == 'dev'
-Requires-Dist: moto (==4.1.3) ; extra == 'dev'
-Requires-Dist: pyre-extensions (==0.0.21) ; extra == 'dev'
+Requires-Dist: mlflow-skinny ; extra == 'dev'
+Requires-Dist: moto (==4.1.6) ; extra == 'dev'
+Requires-Dist: pyre-extensions ; extra == 'dev'
+Requires-Dist: pyre-check ; extra == 'dev'
 Requires-Dist: pytest ; extra == 'dev'
 Requires-Dist: pytorch-lightning (==1.5.10) ; extra == 'dev'
 Requires-Dist: torch-model-archiver (>=0.4.2) ; extra == 'dev'
 Requires-Dist: torch (>=1.10.0) ; extra == 'dev'
 Requires-Dist: torchmetrics (<0.11.0) ; extra == 'dev'
 Requires-Dist: torchserve (>=0.4.2) ; extra == 'dev'
 Requires-Dist: torchtext (>=0.11.0) ; extra == 'dev'
@@ -83,14 +85,15 @@
 
 * Kubernetes (EKS, GKE, AKS, etc)
 * Slurm
 * AWS Batch
 * Docker
 * Local
 * Ray (prototype)
+* GCP Batch (prototype)
 
 Need a scheduler not listed? [Let us know!](https://github.com/pytorch/torchx/issues?q=is%3Aopen+is%3Aissue+label%3Ascheduler-request)
 
 ## Quickstart
 
 See the [quickstart guide](https://pytorch.org/torchx/latest/quickstart.html).
 
@@ -125,14 +128,17 @@
 pip install "torchx[kfp]"
 
 # install torchx Kubernetes / Volcano support
 pip install "torchx[kubernetes]"
 
 # install torchx Ray support
 pip install "torchx[ray]"
+
+# install torchx GCP Batch support
+pip install "torchx[gcp_batch]"
 ```
 
 ### Nightly
 
 ```bash
 # install torchx sdk and CLI
 pip install torchx-nightly[dev]
```

## Comparing `codeflare_torchx-0.5.0.dev5.dist-info/RECORD` & `codeflare_torchx-0.6.0.dev0.dist-info/RECORD`

 * *Files 10% similar despite different names*

```diff
@@ -1,113 +1,117 @@
 torchx/__init__.py,sha256=ff7M2dxFLin47E152U1gSdfLog0UFD-dIlB1lX033es,340
 torchx/notebook.py,sha256=6x9VlBjiOpwB9aCnRiGz40Nmd_Kxj2wxQp9t0HW-Zg4,993
-torchx/version.py,sha256=kZUtioHm8oE5QlB4AdsGht3FSftXQJLjzzpmctC2Inc,935
+torchx/version.py,sha256=ThwRZDHoAearRXw_xbuKgaW9xudZ_E3GT5tgb7c4euo,936
 torchx/apps/__init__.py,sha256=fE0IHi1JJpxsNVBNzWNee2thrNXFFRhY94c80RxNSIE,231
 torchx/apps/serve/__init__.py,sha256=Md3cCHD7Ano9kV15PqGbicgUO-RMdh4aVy1yKiDt_xE,208
 torchx/apps/serve/serve.py,sha256=2LxjeE_d1qCM2bxltDwYXjKbgcizm2YaIc_qPC2akvo,4371
 torchx/apps/utils/__init__.py,sha256=Md3cCHD7Ano9kV15PqGbicgUO-RMdh4aVy1yKiDt_xE,208
 torchx/apps/utils/booth_main.py,sha256=qZn-ThyHfWXz2z901pX5qbJCT75APzplYOb_GawIj18,1412
 torchx/apps/utils/copy_main.py,sha256=EoRHatzahLLsJmQXJL4HJgDycH8y_U5EZrkns1Fbyao,1823
 torchx/apps/utils/process_monitor.py,sha256=2-RcFX6Rn-9LSQ8dUWiS13F4b5aM8dsqyT0nnyRjslM,3437
 torchx/cli/__init__.py,sha256=lah8hAdRGyQcz_CQ8SRE3kQG7BLuNVeNLi448sHJpSo,10336
-torchx/cli/argparse_util.py,sha256=0nrgxx0ERBGQpynG-E4CLj8JFjFTGXI9pfsagPoeceY,2947
+torchx/cli/argparse_util.py,sha256=WhOu5OGI5SMyKCDIxxPKMCfS5RwztiWbVIzH7KXpAWg,2836
 torchx/cli/cmd_base.py,sha256=v6sjnk2yrPRKaJ2sjKILCY8gLNMxQrfHK4xVpjkfCoc,775
 torchx/cli/cmd_cancel.py,sha256=MDIrJ26Id5KCd10uIM-nHh77hIa8GbM_m0bVWkogjVU,820
-torchx/cli/cmd_configure.py,sha256=84GEJufEac5hZNU5ojnKBfVrDmjiEbU_eXhpMrKctbM,1708
+torchx/cli/cmd_configure.py,sha256=PSDjkG-EuRalG0t2CZ9qB68wNHYPsc66VT3PNRC9LJU,1707
 torchx/cli/cmd_describe.py,sha256=s6Qw6RDSibJeP0Flfw02QFqbhwvc57l9DYja4tmklkw,1268
 torchx/cli/cmd_list.py,sha256=zwhGQro9UgNjBMF6demc3nMp_pqPxNnH5cdtoGDZSZU,1414
 torchx/cli/cmd_log.py,sha256=nuN6K0eRaFCxH6aJM-gX3l8-YGPCvSr9ThD7eMP-dvI,6003
-torchx/cli/cmd_run.py,sha256=xKlOzQi9yDnD6LTDAnDjbLUi9QzZMg4QjkjhYH_Z7rM,10507
+torchx/cli/cmd_run.py,sha256=ITZ8u9EW3aYeV9l6EV-TqodLLh354Q57xOCMd-oaUwE,10417
 torchx/cli/cmd_runopts.py,sha256=smQEJChk678o9n1y-crsVFiTMICp1C0PXcwoC3nRCtU,1287
-torchx/cli/cmd_status.py,sha256=BOk8gT2M2kg16vvyeEWPsP6g8pvEz1On9vhHxqDjDKM,1827
-torchx/cli/cmd_tracker.py,sha256=Lc8RCDch7xa6yv0PzgcjGmVoo3yJWy9NmlqxsMCe7Do,6183
+torchx/cli/cmd_status.py,sha256=BG-ScFn1wpLnf_VWphFvv-OI24JRQIbpCQ3_vPVVtG4,1821
+torchx/cli/cmd_tracker.py,sha256=S4GOTtBWoR3sbAJxRzjrF_I5uPvUjjoX-S4YEgGBLI0,5203
 torchx/cli/colors.py,sha256=bVN_jEDwLgvypnDMeCHKn0q0ZDDhQjBJnyVfZHAE6nc,553
-torchx/cli/main.py,sha256=ysAQh0vPn0hC3JeVzRP0i7-E6dTO2D4rLN2B5Ok3abw,3442
+torchx/cli/main.py,sha256=DJYikTWacADa4VoscqZGjZmMKWWK29tBl6-pGtnzsRE,3469
 torchx/components/__init__.py,sha256=6-TQ4SY-Tn56os_1lOs_HMabOoE7gkkud_8e1BgvfJw,12106
-torchx/components/component_test_base.py,sha256=rsy9eG8AC7KpH9LviqJbnHYg9i3F_teNUldhTiG6r40,3995
-torchx/components/dist.py,sha256=8o348h5vZ8Kf6BWlxEDAKmLC4_96Qr5w_H7SDqyLN6E,9521
+torchx/components/component_test_base.py,sha256=eKOwBp5cRgiA4FgZd_FCvyJ-ppv2v3JN9AGXnaSK_Cw,4135
+torchx/components/dist.py,sha256=T1FHJhhinAuuPh4I2kZmzN71eiENq1Pt-dEq_9SfQwY,14284
 torchx/components/interpret.py,sha256=g8gkKdDJvsBfX1ZrpVT7n2bMEtmwRV_1AqDyAnnQ_aA,697
 torchx/components/metrics.py,sha256=1gbp8BfzZWGa7PD1db5vRADlONzmae4qSBUUdCWayr0,2814
 torchx/components/serve.py,sha256=9RlpwlU2KOC7sMOZBeYwUpJIKDCXrU8xNo1SH-AT3fc,2141
+torchx/components/structured_arg.py,sha256=uavcUeFDRnMP7cWAqcxR3ujJYi6JEsClz0_Rd4Dgxj4,9542
 torchx/components/train.py,sha256=vtrQXRcD7bIcbb3lSeyD9BBlIe1mv1WNW6rnLK9R0Mw,1259
 torchx/components/utils.py,sha256=m7mFe6du2AMHpViwcW9dF8rr_twQB6KHQuEzJyHwBXw,9025
 torchx/components/integration_tests/__init__.py,sha256=Md3cCHD7Ano9kV15PqGbicgUO-RMdh4aVy1yKiDt_xE,208
-torchx/components/integration_tests/component_provider.py,sha256=fcdVWJt6dewv16qHOLl4PKQUK1PSfkPWy7d4dKMqZmo,3925
+torchx/components/integration_tests/component_provider.py,sha256=FTwzCrr3p4SWYbxcjN4E6i7qjklyrWgDWL3UGkhoDaA,3980
 torchx/components/integration_tests/integ_tests.py,sha256=OVgRvGrLWhDUNlqbbYj90ukGmkAwka2KubCWUR8pC7Y,5150
+torchx/distributed/__init__.py,sha256=OAO1CIwVBOaclzbp2NjH_SMBq2WlK7aE9NVlNmDtVlQ,8786
 torchx/examples/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 torchx/examples/apps/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 torchx/examples/apps/datapreproc/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 torchx/examples/apps/datapreproc/datapreproc.py,sha256=7GV37WS3JLueID8vGFvqO93ua0PSSx__hz2MYNto53Q,4302
 torchx/examples/apps/lightning/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-torchx/examples/apps/lightning/data.py,sha256=r7YVX7FWGe0-xCelpOLB0LJz_rgVbA5InZPMh86qy78,6582
+torchx/examples/apps/lightning/data.py,sha256=uTZM_G8Wd-UjzlmeDIsl645qZl9qD7d-cesRFt0nnm4,6583
 torchx/examples/apps/lightning/interpret.py,sha256=Hd3kE5a6FyhxCmJBfTzb4Tlj518zhX8V0XvZfzu4nqE,5256
 torchx/examples/apps/lightning/model.py,sha256=GQpn4SDdwYxsgD4oZLynZCqPB8emd_veB5feYFqEVZ8,3932
 torchx/examples/apps/lightning/profiler.py,sha256=SWl4deBqZVf-K8Eojoelo7P04AnUIE4ig03Tjoo3qCE,1926
 torchx/examples/apps/lightning/train.py,sha256=_MTFX0llRbgNMNoy6v7ryWjPxHZ9AM2Xz270sicMx0g,6084
 torchx/examples/pipelines/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 torchx/examples/pipelines/kfp/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 torchx/examples/pipelines/kfp/advanced_pipeline.py,sha256=bMBk3bZMl7TPGyE8I3GJHgucsUjxZqshc9W57PzyrzY,8416
 torchx/examples/pipelines/kfp/dist_pipeline.py,sha256=hMAVKuwmCTZN0c2lHkkix7gSFS5wHePH-elaWijaJa4,2178
 torchx/examples/pipelines/kfp/intro_pipeline.py,sha256=wAkk0DoGSbJRNlti7gLdaudIzWiqXC4SlqVerYcphFY,2751
 torchx/pipelines/__init__.py,sha256=2MbRVk5xwRjg-d2qPemeXpEhDsocMQumPQ53lsesZAI,606
 torchx/pipelines/kfp/__init__.py,sha256=uDP4O5xrV23ncb79cNw7ifWUWJl6Qy2-d_BQyfQnTRw,721
-torchx/pipelines/kfp/adapter.py,sha256=cFfiaOWKZQHW2t4CvDzmwiMdyWAhodzQgfqTOCmugfk,8942
+torchx/pipelines/kfp/adapter.py,sha256=AKC86EuhoF-KMG6Lc3t0B3TIy0AbNLTwKL5qng-YVoY,8952
 torchx/pipelines/kfp/version.py,sha256=4422bXkqH2qFTTv-Vh16Cu4EO9DrKYJTl2GpMmC9kDg,524
 torchx/runner/__init__.py,sha256=YYJP4xys6f_hKCBLQkqrCAmjhgXGlXuAVNf4ozSLu70,300
-torchx/runner/api.py,sha256=z3e6Uy-6H7vEF3C_2I96h5gnTF9JdEd3lBhk-H6DnDc,27108
-torchx/runner/config.py,sha256=skQRinXlvVwgHugHxV1kyoGwR-VS1hBYtpjBPgZ6C2I,17013
+torchx/runner/api.py,sha256=CJVkVL9WJf653-_nsq2hWh7kBPFtMEec6b3k0oiI4Ag,26817
+torchx/runner/config.py,sha256=rt2rpKa6E1pv7IqtU57O7AmSI65HkQCKXAeIOit4XEw,17171
 torchx/runner/events/__init__.py,sha256=L0atOje4p1go7LvTe77qkZSK2eqewipg-3gQ1digibw,3592
 torchx/runner/events/api.py,sha256=Amj9aamjh3AUo4iGzcRO-bxfmtsWL1fdwj2M77PHhIE,1957
 torchx/runner/events/handlers.py,sha256=esLkzce9fvwgjqu1IrO_furmhECUqwUeL7wy5zbqtUo,507
 torchx/runtime/__init__.py,sha256=Wxje2BryzeQneFu5r6P9JJiEKG-_C9W1CcZ_JNrKT6g,593
 torchx/runtime/tracking/__init__.py,sha256=uHbJ1NqsxFWGYz2aV0_p4OCMhW467zDJu_86B0C1Mn8,3040
 torchx/runtime/tracking/api.py,sha256=9mlsCnnKP8hfvypNcEX2_57OYMW4AuTMM-nvsIgOzK4,5457
 torchx/schedulers/__init__.py,sha256=cCansxGU45SV_lxhgzyw2on7AJyIvhprAFo6Di1x9xQ,2157
 torchx/schedulers/api.py,sha256=BtDK4P20q38aHoMboNfqY4MnvMSonUz25mYt9Y8Bu74,13941
-torchx/schedulers/aws_batch_scheduler.py,sha256=BuKivG9dEIGpIB7cw6VnsMPfhpO9OM-KnDcbtrBeLjE,21954
+torchx/schedulers/aws_batch_scheduler.py,sha256=_jetOwBgxBr4F0jpaUanhHClRamT0uEctXqVnc_W4W4,26063
 torchx/schedulers/devices.py,sha256=PNbcpf8fEM18Ag1RgK9Q30zPBalEcPdsFWctdbLxuv8,1352
-torchx/schedulers/docker_scheduler.py,sha256=d7RjnZg_rMANXj5OPzob-mprxokVlG_iPVIMIl5L89c,15247
+torchx/schedulers/docker_scheduler.py,sha256=DU5YdLrVCjyoVxqpqeUM4ssTISuMEQaDGOBHU-cqcms,15370
 torchx/schedulers/gcp_batch_scheduler.py,sha256=MNlZGjNZVd-E7m7sR5DN0BidRJOU0w8TZDpovdU3d9o,15993
 torchx/schedulers/ids.py,sha256=IGsJEbCYTdfKdU3MhKLQU6b7sWCJy5dlRV6JIL_9BlE,1783
-torchx/schedulers/kubernetes_mcad_scheduler.py,sha256=fFfcwC6x7fe1G48cwHaDQb9LNMk7PKoZ27gTNCnfPbg,38388
-torchx/schedulers/kubernetes_scheduler.py,sha256=4T1FtAhiIql4ZLZ2Xr55Z3nXCwAiv1Png8SXxP9c9ss,26345
-torchx/schedulers/local_scheduler.py,sha256=LKbWjw4cJuoryvaBjbHXVOVeTYOXaNDcKk61Grjg8Eg,37442
-torchx/schedulers/lsf_scheduler.py,sha256=SwstV-aBc8ZwAsjOViNiPfQX9iVLFmMDBba3Q2JglIE,17538
-torchx/schedulers/ray_scheduler.py,sha256=MXAuoqRVFxl_vZ6Mn98NucUMCgE3juDoOou07Kci8So,16235
+torchx/schedulers/kubernetes_mcad_scheduler.py,sha256=Q4RWGplSvZHvK4i-BOlxfOVke0bTOTmddl4DomqmnS0,41100
+torchx/schedulers/kubernetes_scheduler.py,sha256=bkfrR6arwgsLHoZaGkGu--f3WtnbaTOfnd7Fa0j_pVo,26830
+torchx/schedulers/local_scheduler.py,sha256=V-Qox7jIbrr-vGaWrOeU6NYds47mDlA1EQcMj4ehbK4,37591
+torchx/schedulers/lsf_scheduler.py,sha256=3lRSY9yiM83YFDHR73KVqgaQDMJ3Hsi2wRUl-Ak3u80,17552
+torchx/schedulers/ray_scheduler.py,sha256=DDty_EysNAT7EWzBqEkpvtec8J6pms6btt0_pmwojg0,16234
 torchx/schedulers/slurm_scheduler.py,sha256=32lW_lld1-U6tp8UBoscEHtf-ooplSzuZ-GkrufP1Xg,19269
 torchx/schedulers/streams.py,sha256=ObaKwEEcnsjrPyc6VZOp8cgZ_f2RFextAxeISxZUWeQ,1992
 torchx/schedulers/ray/__init__.py,sha256=fE0IHi1JJpxsNVBNzWNee2thrNXFFRhY94c80RxNSIE,231
 torchx/schedulers/ray/ray_common.py,sha256=pyNYFvTKVwdjDAeCBNbPwAWwVNmlLOJWExfn90XY8u8,610
 torchx/schedulers/ray/ray_driver.py,sha256=aMq65USzeyozSUoJyMWlSDC3ZOfGG_LqQgmQRPwUCeA,12127
-torchx/specs/__init__.py,sha256=lWdg9f9OVtJ5QmgMO_o6ek9N3gTb8XimP0HIlJVrZcY,5369
+torchx/specs/__init__.py,sha256=fSA89Y0ZpdZLJmhIfEKNbjNNi6fbDR9k1bpIM7Xm7xo,5462
 torchx/specs/api.py,sha256=vFtvhYW18HvWzET8ob8ONNRC1MCuYwLMXohWDy0LbZI,33798
 torchx/specs/builders.py,sha256=dsKa80PD-cuIjXhFF2TsEY0eL8S01wux8aZaQrURIEE,8512
 torchx/specs/file_linter.py,sha256=LREWELpHJyE7YN3rc5ixf2ZydWFU9dlcSy5gGqdB5rA,11714
-torchx/specs/finder.py,sha256=jVPfC1QkZ82-0X2CJ6QUBZKloSgNtWK_N1h2BTren14,16128
+torchx/specs/finder.py,sha256=RJI0PkG69esuVzhCp4w6Lotu2tSzIRh6PhWemCSQR7I,16234
 torchx/specs/named_resources_aws.py,sha256=6ID0jOGi5HVXn9BLylzCMD-SUQtxGeBHS5zU8PCten4,6361
+torchx/specs/named_resources_generic.py,sha256=_xz0cRjy3fz-CVtX9G_MY7f3NX6n3AkP3xzAkuDevwk,2631
 torchx/specs/test/components/__init__.py,sha256=Md3cCHD7Ano9kV15PqGbicgUO-RMdh4aVy1yKiDt_xE,208
 torchx/specs/test/components/a/__init__.py,sha256=T7exlQ47Fak5ajCEGPg6_yOfChJCWpIMhWBmSVUnlrQ,546
 torchx/specs/test/components/a/b/__init__.py,sha256=Md3cCHD7Ano9kV15PqGbicgUO-RMdh4aVy1yKiDt_xE,208
 torchx/specs/test/components/a/b/c.py,sha256=QyTZfsCaSZscmk3DeNOkAyMoz6GCcayrWtOKbNFIZ1M,539
 torchx/specs/test/components/c/__init__.py,sha256=fE0IHi1JJpxsNVBNzWNee2thrNXFFRhY94c80RxNSIE,231
 torchx/specs/test/components/c/d.py,sha256=RH07jjo6uvFbzIaNFnAwmD_h24cEsT8kyZDTN-ezFio,531
-torchx/tracker/__init__.py,sha256=kaynfAwMYtdkkvXNrb4Rmin2mSd5vi9mg25iz1e0TJ8,4200
-torchx/tracker/api.py,sha256=vtBszMhDvrl6bYFw7hp6Edrk_It7CT8585WfNtJ4cVw,11299
+torchx/tracker/__init__.py,sha256=bnqiFZmrQoioeDlUkrzxFYzfbuf6OQvrlZmhtNekw8U,4206
+torchx/tracker/api.py,sha256=WRS3As_8YkxaL_enMz60wKba9cC_ynGIp8IQwNBbTTM,11263
+torchx/tracker/mlflow.py,sha256=P_mj7Yi-bZc7QOZ-6PJW4FHeWWGX0rjauNK0zXC25ig,13221
 torchx/tracker/backend/__init__.py,sha256=fE0IHi1JJpxsNVBNzWNee2thrNXFFRhY94c80RxNSIE,231
 torchx/tracker/backend/fsspec.py,sha256=JpSioMgn54mrxqqpY0kw5Gudqx9hhxkgDLaOFSEP2Ko,10425
 torchx/util/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 torchx/util/datetime.py,sha256=e-sO5Wjx1Utpln14C3qfJHl4v3KM-SMnn11hSyvkqFY,390
 torchx/util/entrypoints.py,sha256=C4A7cF1tPLlfyYWyZ7uZEtsKeuoOoLbMv0sOSxLhXs4,2710
 torchx/util/io.py,sha256=sxb6KI42Lq6n5z6_-YKW_mAhgPdC6CxzexlMyGheWSc,1792
 torchx/util/shlex.py,sha256=KzyWektMeU3oXS3Z5mFkNSPLItBTszVcvQ3EYfOMUYA,448
-torchx/util/strings.py,sha256=CfR2FtT0bNFQW-jeILL5gr85u3QShpBDR5_VIJmyYI0,660
+torchx/util/strings.py,sha256=7CZe5WKHa7IQ6DuJCYeJ5FapUC4Fd1OGeq1yZAmjluw,663
 torchx/util/types.py,sha256=6ASuDKGO91UU3DCSuWhPX_C03341tApLCQEByUz8xpY,7016
 torchx/workspace/__init__.py,sha256=KbGEzJqqXaIxALm_EQO64aw-fE7MeDMFXcpU1mY650I,783
 torchx/workspace/api.py,sha256=Ej6DR__mNWaVyZgoVNAAOloDy1kTD5X1jz7pRtoVf80,5464
 torchx/workspace/dir_workspace.py,sha256=Fz-hKIx0KN8iJf2BsthNj0NvTkWlxP6WFsElPs_BaT0,2253
 torchx/workspace/docker_workspace.py,sha256=Yd8ut26bNfjyJQnmH8ANOrflfr-4VKcnOrIjbi_XIUY,9208
-codeflare_torchx-0.5.0.dev5.dist-info/LICENSE,sha256=e0Eotbf_rHOYPuEUlppIbvwy4SN98CZnl_hqwvbDA4Q,1530
-codeflare_torchx-0.5.0.dev5.dist-info/METADATA,sha256=sazpOEoJ13mYJoDu1erdbdtUGOTu_G4tLfXnas81CvI,5327
-codeflare_torchx-0.5.0.dev5.dist-info/WHEEL,sha256=pkctZYzUS4AYVn6dJ-7367OJZivF2e8RA9b_ZBjif18,92
-codeflare_torchx-0.5.0.dev5.dist-info/entry_points.txt,sha256=T328AMXeKI3JZnnxfkEew2ZcMN1oQDtkXjMz7lkV-P4,169
-codeflare_torchx-0.5.0.dev5.dist-info/top_level.txt,sha256=pxew3bc2gsiViS0zADs0jb6kC5v8o_Yy_85fhHj_J1A,7
-codeflare_torchx-0.5.0.dev5.dist-info/RECORD,,
+codeflare_torchx-0.6.0.dev0.dist-info/LICENSE,sha256=WVHfXhFC0Ia8LTKt_nJVYobdqTJVg_4J3Crrfm2A8KQ,1721
+codeflare_torchx-0.6.0.dev0.dist-info/METADATA,sha256=zacda9vyIZ3t8JIQ7-anbvXcT7du-LsLbCzYqWjRF3s,5497
+codeflare_torchx-0.6.0.dev0.dist-info/WHEEL,sha256=pkctZYzUS4AYVn6dJ-7367OJZivF2e8RA9b_ZBjif18,92
+codeflare_torchx-0.6.0.dev0.dist-info/entry_points.txt,sha256=T328AMXeKI3JZnnxfkEew2ZcMN1oQDtkXjMz7lkV-P4,169
+codeflare_torchx-0.6.0.dev0.dist-info/top_level.txt,sha256=pxew3bc2gsiViS0zADs0jb6kC5v8o_Yy_85fhHj_J1A,7
+codeflare_torchx-0.6.0.dev0.dist-info/RECORD,,
```

