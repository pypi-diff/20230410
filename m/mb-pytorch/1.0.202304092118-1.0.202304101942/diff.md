# Comparing `tmp/mb_pytorch-1.0.202304092118-py3-none-any.whl.zip` & `tmp/mb_pytorch-1.0.202304101942-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,18 +1,18 @@
-Zip file size: 28205 bytes, number of entries: 36
--rw-rw-r--  2.0 unx     2548 b- defN 23-Apr-08 10:22 mb_pytorch/classification/training.py
+Zip file size: 28413 bytes, number of entries: 36
+-rw-rw-r--  2.0 unx     3133 b- defN 23-Apr-10 19:17 mb_pytorch/classification/training.py
 -rw-rw-r--  2.0 unx       44 b- defN 23-Mar-16 11:39 mb_pytorch/dataloader/__init__.py
 -rw-rw-r--  2.0 unx    11844 b- defN 23-Mar-31 20:26 mb_pytorch/dataloader/loader.py
 -rw-rw-r--  2.0 unx        0 b- defN 23-Feb-23 13:56 mb_pytorch/metalearning/__init__.py
 -rw-rw-r--  2.0 unx     1385 b- defN 23-Mar-02 03:28 mb_pytorch/metalearning/meta_utils.py
 -rw-rw-r--  2.0 unx     1030 b- defN 23-Mar-15 02:58 mb_pytorch/metalearning/proto_dataloader.py
 -rw-rw-r--  2.0 unx     2861 b- defN 23-Mar-03 23:55 mb_pytorch/metalearning/prototypical.py
 -rw-rw-r--  2.0 unx        0 b- defN 23-Mar-31 19:54 mb_pytorch/models/__init__.py
 -rw-rw-r--  2.0 unx      920 b- defN 23-Mar-12 03:33 mb_pytorch/models/lenet.py
--rw-rw-r--  2.0 unx     2933 b- defN 23-Apr-01 03:27 mb_pytorch/models/modelloader.py
+-rw-rw-r--  2.0 unx     3075 b- defN 23-Apr-10 19:23 mb_pytorch/models/modelloader.py
 -rw-rw-r--  2.0 unx    10784 b- defN 23-Apr-01 03:27 mb_pytorch/models/unet_models.py
 -rw-rw-r--  2.0 unx        0 b- defN 23-Mar-31 19:54 mb_pytorch/models/blocks/__init__.py
 -rw-rw-r--  2.0 unx     2801 b- defN 23-Apr-01 00:43 mb_pytorch/models/blocks/attention_block.py
 -rw-rw-r--  2.0 unx     3629 b- defN 23-Mar-23 00:49 mb_pytorch/models/blocks/cnn.py
 -rw-rw-r--  2.0 unx     4783 b- defN 23-Mar-23 04:50 mb_pytorch/models/blocks/conv_block.py
 -rw-rw-r--  2.0 unx      920 b- defN 23-Mar-17 22:06 mb_pytorch/models/blocks/conv_with_relu.py
 -rw-rw-r--  2.0 unx     2256 b- defN 23-Mar-23 14:45 mb_pytorch/models/blocks/model_out.py
@@ -25,14 +25,14 @@
 -rw-rw-r--  2.0 unx      257 b- defN 23-Mar-01 22:55 mb_pytorch/utils/dist.py
 -rw-rw-r--  2.0 unx     2850 b- defN 23-Mar-27 17:33 mb_pytorch/utils/extra_utils.py
 -rw-rw-r--  2.0 unx     7178 b- defN 23-Mar-15 02:58 mb_pytorch/utils/generate_emb.py
 -rw-rw-r--  2.0 unx     2582 b- defN 23-Apr-03 19:30 mb_pytorch/utils/losses.py
 -rw-rw-r--  2.0 unx     1199 b- defN 23-Apr-04 20:14 mb_pytorch/utils/metrics.py
 -rw-rw-r--  2.0 unx     5537 b- defN 23-Mar-27 17:38 mb_pytorch/utils/viewer.py
 -rw-rw-r--  2.0 unx      994 b- defN 23-Mar-06 13:11 mb_pytorch/utils/yaml_reader.py
--rwxrwxr-x  2.0 unx     1304 b- defN 23-Apr-09 21:19 mb_pytorch-1.0.202304092118.data/scripts/dataload_results.py
--rwxrwxr-x  2.0 unx      980 b- defN 23-Mar-15 02:59 mb_pytorch-1.0.202304092118.data/scripts/emb.py
--rw-rw-r--  2.0 unx      329 b- defN 23-Apr-09 21:19 mb_pytorch-1.0.202304092118.dist-info/METADATA
--rw-rw-r--  2.0 unx       92 b- defN 23-Apr-09 21:19 mb_pytorch-1.0.202304092118.dist-info/WHEEL
--rw-rw-r--  2.0 unx       11 b- defN 23-Apr-09 21:19 mb_pytorch-1.0.202304092118.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx     3256 b- defN 23-Apr-09 21:19 mb_pytorch-1.0.202304092118.dist-info/RECORD
-36 files, 80763 bytes uncompressed, 22885 bytes compressed:  71.7%
+-rwxrwxr-x  2.0 unx     1304 b- defN 23-Apr-10 19:42 mb_pytorch-1.0.202304101942.data/scripts/dataload_results.py
+-rwxrwxr-x  2.0 unx      980 b- defN 23-Mar-15 02:59 mb_pytorch-1.0.202304101942.data/scripts/emb.py
+-rw-rw-r--  2.0 unx      329 b- defN 23-Apr-10 19:42 mb_pytorch-1.0.202304101942.dist-info/METADATA
+-rw-rw-r--  2.0 unx       92 b- defN 23-Apr-10 19:42 mb_pytorch-1.0.202304101942.dist-info/WHEEL
+-rw-rw-r--  2.0 unx       11 b- defN 23-Apr-10 19:42 mb_pytorch-1.0.202304101942.dist-info/top_level.txt
+?rw-rw-r--  2.0 unx     3256 b- defN 23-Apr-10 19:42 mb_pytorch-1.0.202304101942.dist-info/RECORD
+36 files, 81490 bytes uncompressed, 23093 bytes compressed:  71.7%
```

## zipnote {}

```diff
@@ -84,26 +84,26 @@
 
 Filename: mb_pytorch/utils/viewer.py
 Comment: 
 
 Filename: mb_pytorch/utils/yaml_reader.py
 Comment: 
 
-Filename: mb_pytorch-1.0.202304092118.data/scripts/dataload_results.py
+Filename: mb_pytorch-1.0.202304101942.data/scripts/dataload_results.py
 Comment: 
 
-Filename: mb_pytorch-1.0.202304092118.data/scripts/emb.py
+Filename: mb_pytorch-1.0.202304101942.data/scripts/emb.py
 Comment: 
 
-Filename: mb_pytorch-1.0.202304092118.dist-info/METADATA
+Filename: mb_pytorch-1.0.202304101942.dist-info/METADATA
 Comment: 
 
-Filename: mb_pytorch-1.0.202304092118.dist-info/WHEEL
+Filename: mb_pytorch-1.0.202304101942.dist-info/WHEEL
 Comment: 
 
-Filename: mb_pytorch-1.0.202304092118.dist-info/top_level.txt
+Filename: mb_pytorch-1.0.202304101942.dist-info/top_level.txt
 Comment: 
 
-Filename: mb_pytorch-1.0.202304092118.dist-info/RECORD
+Filename: mb_pytorch-1.0.202304101942.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## mb_pytorch/classification/training.py

```diff
@@ -1,48 +1,58 @@
 from ..models.modelloader import ModelLoader
 from ..dataloader.loader import DataLoader
 import torch
 from ..training.train_params import train_helper
 import tqdm
 from torch.utils.tensorboard import SummaryWriter
 import os
+from mb_utils.src.logging import logger
 
 yaml_file = '/home/malav/mb_pytorch/scripts/models/loader_y.yaml'
 data = DataLoader(yaml_file,logger=None)
 data_model = data.data_dict['model']
 train_loader, val_loader,_,_ = data.data_load(logger=None)
 model = ModelLoader(data_model,logger=None)
 
 device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
 
 loss_attr,optimizer_attr,optimizer_dict,scheduler_attr,scheduler_dict = train_helper(data_model)
 
-optimizer_dict['lr'] = scheduler_attr.get_last_lr()[0]
-optimizer =optimizer_attr(model.parameters(),*optimizer_dict)
-scheduler = scheduler_attr(optimizer,*scheduler_dict)
+#optimizer_dict['lr'] = scheduler_attr.get_last_lr()[0]
+optimizer =optimizer_attr(model.parameters(),**optimizer_dict)
+scheduler = scheduler_attr(optimizer,**scheduler_dict)
 
 best_val_loss = float('inf')
 
 path_logs = os.path.join(data['data']['work_dir'], 'logs')
 writer = SummaryWriter(log_dir=path_logs)
 
+logger = logger
 
-for i in tqdm(range(data_model['model_epochs'])):
+
+for i in tqdm.tqdm(range(data_model['model_epochs'])):
     ##train loop
     model.train()
     train_loss = 0
+    logger.info('Training Started')
     for j,(x,y) in enumerate(train_loader):
         x,y = x.to(device),y.to(device)
         optimizer.zero_grad()
         y_pred = model(x)
-        current_loss = loss_attr(y_pred,y)
+        current_loss = loss_attr()(y_pred,y)
         current_loss.backward()    
         optimizer.step()
-        
-    writer.add_scalar('Loss/train', loss_attr.item(), global_step=i)
+        train_loss += current_loss.item()
+        logger.info(f'Epoch {i+1} - Batch {j+1} - Train Loss: {current_loss.item()}')
+
+    avg_train_loss = train_loss / len(train_loader)
+    logger.info(f'Epoch {i+1} - Train Loss: {avg_train_loss}')
+    
+    writer.add_scalar('Loss/train', loss_attr().item(), global_step=i)
+    
     if scheduler is not None:
         scheduler.step()
     
     for name, param in model.named_parameters():
         writer.add_histogram(name, param, global_step=i)
         
     #get grad cam images
@@ -53,28 +63,31 @@
     num_samples = 0
     
     model.eval()
     with torch.no_grad():
         for x_val, y_val in val_loader:
             x_val, y_val = x_val.to(device), y_val.to(device)
             output = model(x_val)
-            val_loss += loss_attr(output, y_val).item() * x_val.size(0)
+            val_loss += loss_attr()(output, y_val).item() * x_val.size(0)
             _, preds = torch.max(output, 1)
             val_acc += torch.sum(preds == y_val.data)
             num_samples += x_val.size(0)
+            logger.info(f'Epoch {i+1} - Batch {j+1} - Val Loss: {val_loss}')
             
         val_loss /= num_samples
         val_acc = val_acc.double() / num_samples
+        logger.info(f'Epoch {i+1} - Val Loss: {val_loss}', f'Epoch {i+1} - Val Accuracy: {val_acc}')
     
     writer.add_scalar('Loss/val', val_loss, global_step=i)
     writer.add_scalar('Accuracy/val', val_acc, global_step=i)
     
     # save best model
     if val_loss < best_val_loss:
         best_val_loss = val_loss
         best_model = model.state_dict()
 
         path = os.path.join(data['data']['work_dir'], 'best_model.pth')
         torch.save(best_model, path)
+        logger.info(f'Epoch {i+1} - Best Model Saved')
         
     model.train()
```

## mb_pytorch/models/modelloader.py

```diff
@@ -31,21 +31,24 @@
         self._data= data 
         self._use_torchvision_models=self._data['use_torchvision_models']
         self._model_name=self._data['model_name']
         self._model_version=self._data['model_version']
         self._model_path=self._data['model_path']
         self._model_pretrained=self._data['model_pretrained']
         self._load_model = self._data['load_model']
+        self._model_num_classes = self._data['model_num_classes']
 
     def model_type(self):
         """
         Function to get default model resnet, vgg, densenet, googlenet, inception, mobilenet, mnasnet, shufflenet_v2, squeezenet
         """
         model_final = self._model_name + self._model_version
-        model_out = getattr(torchvision.models,model_final)(pretrained=self._model_pretrained)            
+        model_out = getattr(torchvision.models,model_final)(pretrained=self._model_pretrained)
+        num_ftrs = model_out.fc.in_features
+        model_out.fc = nn.Linear(num_ftrs, self._model_num_classes)            
         return model_out
 
     def model_params(self):
         """
         Function to pass the model params to custom model
         """        
         #check if model is available in the models list
@@ -56,15 +59,14 @@
     def get_model(self):
         """
         FUnction to get the model
         """
         # Check if the model is available in torchvision models
 
         if self._load_model:
-            print(self._load_model)
             self.model = torch.load(self._data['load_model'])
             return self.model
 
         if self._use_torchvision_models:
             try:
                 # Try to load the model from the specified path
                 if hasattr(models, self._model_name):
```

## Comparing `mb_pytorch-1.0.202304092118.data/scripts/dataload_results.py` & `mb_pytorch-1.0.202304101942.data/scripts/dataload_results.py`

 * *Files identical despite different names*

## Comparing `mb_pytorch-1.0.202304092118.data/scripts/emb.py` & `mb_pytorch-1.0.202304101942.data/scripts/emb.py`

 * *Files identical despite different names*

## Comparing `mb_pytorch-1.0.202304092118.dist-info/RECORD` & `mb_pytorch-1.0.202304101942.dist-info/RECORD`

 * *Files 10% similar despite different names*

```diff
@@ -1,17 +1,17 @@
-mb_pytorch/classification/training.py,sha256=ZOHZw2fg2QYl3fkwTstMNcnJxEfZVkzDyPtfyH3QhEc,2548
+mb_pytorch/classification/training.py,sha256=YSNb0UCcAyTy68qRxztNTuKXfqUl8mwwziwtJ0Ed-og,3133
 mb_pytorch/dataloader/__init__.py,sha256=nB0xPAHbI91Ra1dDkWR1l4td5A4k9xko-I5Jdgv5apI,44
 mb_pytorch/dataloader/loader.py,sha256=NQMZszSD8WN0Ile1NY9BlpgjXmNxUm86H9KefxwKUJM,11844
 mb_pytorch/metalearning/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 mb_pytorch/metalearning/meta_utils.py,sha256=mgHYiQIIcYQ1pVTJcrjquSXpQstdYD1q8iXO09Zao1s,1385
 mb_pytorch/metalearning/proto_dataloader.py,sha256=WvrfZYkYMxorocCkR_zHS_AC8W_ML9YndB-P6evkdcc,1030
 mb_pytorch/metalearning/prototypical.py,sha256=qFVf6VF3s8zskGqbM3geJV-dfkdO3tRaf3P8U_KR-cE,2861
 mb_pytorch/models/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 mb_pytorch/models/lenet.py,sha256=vZpN0LsfVGNI6z91YO3GLPCVB9Sv0EAxaTwEavRSKKo,920
-mb_pytorch/models/modelloader.py,sha256=iwRRyx4kTjOP05072VZ_L7nEIS_A2Ph4gvPqOCMt82I,2933
+mb_pytorch/models/modelloader.py,sha256=fLT6AF0cY2bU8BxEbsDJswLaegST1yZhbeBeORBHA1U,3075
 mb_pytorch/models/unet_models.py,sha256=8AYw1MaFRUPLkfyyPmAaVZol2lRY28oiTjZRgZVRa4s,10784
 mb_pytorch/models/blocks/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 mb_pytorch/models/blocks/attention_block.py,sha256=jMkfI79upW-PDc28dQ17s34gUR0gCpMB0XIpGgFDE_Q,2801
 mb_pytorch/models/blocks/cnn.py,sha256=IFPDIkKb-LyfzqK9RshqzNidSDdfrRmMylADYvWGACE,3629
 mb_pytorch/models/blocks/conv_block.py,sha256=vlcJiBwkbbmhQMpGLzQA0S14Wkw1Ai9adSbOquFBk0c,4783
 mb_pytorch/models/blocks/conv_with_relu.py,sha256=WIwJBfzvp-kE8QY6Ts5wg0ioJAcJIvmqKLpSl46j_II,920
 mb_pytorch/models/blocks/model_out.py,sha256=JmtY1zHCtkj-RbEVhQ8e6EvXd5WWVLrP9Lb_FigO7rA,2256
@@ -24,13 +24,13 @@
 mb_pytorch/utils/dist.py,sha256=7-ZdntmiugRWYnT5wileo8mYTuV1dbjVl4ffJsfnfAw,257
 mb_pytorch/utils/extra_utils.py,sha256=zWWCJUBdPLLx3H5C4-N-DaryYk8VYrU0x7LvSunJ5cg,2850
 mb_pytorch/utils/generate_emb.py,sha256=2iK8wRIrYfaLpEgjdbFnDqGU5ux-1JhncQoeboW_6LQ,7178
 mb_pytorch/utils/losses.py,sha256=OLCPLkJH46IofSSVly2xdcklVv7Q5OFFEGtVrJcV7V0,2582
 mb_pytorch/utils/metrics.py,sha256=Kqmdu9llSjR8aRp3IVlmy6PqeQexf0ZXjTJUcEtvcfI,1199
 mb_pytorch/utils/viewer.py,sha256=vMV9YiwTqFNvgFtQ-dOCtmwAtDgKomDzFzicVFLmQhc,5537
 mb_pytorch/utils/yaml_reader.py,sha256=Azgr_5qttsH_BBVsCtfccFMvK6IEjTRYhd5qp4S5uzk,994
-mb_pytorch-1.0.202304092118.data/scripts/dataload_results.py,sha256=8IFAH7WX-nSJ7V532rr3Cl7R37v0jhSakw0JCd9dalE,1304
-mb_pytorch-1.0.202304092118.data/scripts/emb.py,sha256=5jSbTGNOhusDTvHZeaTwk4pmJa4HIdkRGd98s0L4Rl0,980
-mb_pytorch-1.0.202304092118.dist-info/METADATA,sha256=89npXQ9e-tDFWiWTBDwaYTRFPeFhMaqc_Nn2Ef3SIr0,329
-mb_pytorch-1.0.202304092118.dist-info/WHEEL,sha256=g4nMs7d-Xl9-xC9XovUrsDHGXt-FT0E17Yqo92DEfvY,92
-mb_pytorch-1.0.202304092118.dist-info/top_level.txt,sha256=2m_aBiEfjq3pZM2NtYSlTqlgoQxH6WaK8_8SsRicIvg,11
-mb_pytorch-1.0.202304092118.dist-info/RECORD,,
+mb_pytorch-1.0.202304101942.data/scripts/dataload_results.py,sha256=8IFAH7WX-nSJ7V532rr3Cl7R37v0jhSakw0JCd9dalE,1304
+mb_pytorch-1.0.202304101942.data/scripts/emb.py,sha256=5jSbTGNOhusDTvHZeaTwk4pmJa4HIdkRGd98s0L4Rl0,980
+mb_pytorch-1.0.202304101942.dist-info/METADATA,sha256=9PNk8FZZF0GX0DWehF513rLzU-f_FPl1xpVwUGVxgdo,329
+mb_pytorch-1.0.202304101942.dist-info/WHEEL,sha256=g4nMs7d-Xl9-xC9XovUrsDHGXt-FT0E17Yqo92DEfvY,92
+mb_pytorch-1.0.202304101942.dist-info/top_level.txt,sha256=2m_aBiEfjq3pZM2NtYSlTqlgoQxH6WaK8_8SsRicIvg,11
+mb_pytorch-1.0.202304101942.dist-info/RECORD,,
```

